{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DockyJ/social_network_calls_Brazil/blob/main/sn1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0V7RHHymgYg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os, csv, collections, datetime\n",
        "import networkx as nx\n",
        "from itertools import islice\n",
        "from math import radians, cos, sin, asin, sqrt, log1p\n",
        "import unicodedata, gc\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import normaltest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Google shared folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oc7msXGlnETT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Utility Functions"
      ],
      "metadata": {
        "id": "2gtt5hXeO_sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_string(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn').lower()"
      ],
      "metadata": {
        "id": "Tc0EKpl8GZze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMJNKznImgYj"
      },
      "outputs": [],
      "source": [
        "def read_ana(file_path_ana):\n",
        "    \"\"\"\n",
        "    Read the antenna locations file and return a dictionary of antenna locations.\n",
        "    \"\"\"\n",
        "    # Load antenna locations\n",
        "    antenna_locations = {}\n",
        "\n",
        "    with open(file_path_ana, 'r') as file:\n",
        "        reader = csv.reader(file, delimiter=';')\n",
        "        next(reader)  # Skip the header\n",
        "        for row in reader:\n",
        "            cell_id, lat, lon = row[0], float(row[3]), float(row[4])\n",
        "            antenna_locations[cell_id] = (lat, lon)\n",
        "    return antenna_locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B59tL11mgYk"
      },
      "outputs": [],
      "source": [
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate the great circle distance between two points\n",
        "    on the earth (specified in decimal degrees)\n",
        "    \"\"\"\n",
        "    # Convert decimal degrees to radians\n",
        "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
        "\n",
        "    # Haversine formula\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
        "    c = 2 * asin(sqrt(a))\n",
        "    return c * 6371 # Radius of earth in kilometers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def home_location_calculation(calls_users_time_filter, significant_users):\n",
        "    \"\"\"\n",
        "    Calculate the home location based on the filtered records.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    home_locations = {}\n",
        "\n",
        "    for user, locations in calls_users_time_filter.items():\n",
        "        if user in significant_users:\n",
        "            most_frequent_cell, max_calls = max(locations.items(), key=lambda item: item[1])\n",
        "            total_calls = sum(locations.values())\n",
        "            if max_calls >= total_calls * 0.5:\n",
        "                home_locations[user] = (most_frequent_cell, max_calls)\n",
        "    return home_locations"
      ],
      "metadata": {
        "id": "QIPfe1l26Iy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_data_to_csv(df, city, city_data, output_suffix):\n",
        "    \"\"\"\n",
        "    Write the data to a CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the main directory and subdirectory paths\n",
        "    main_directory = '/content/drive/MyDrive/network_radius_city_size'\n",
        "    subdirectory = os.path.join(main_directory, output_suffix)\n",
        "\n",
        "    # Create the subdirectory if it does not exist\n",
        "    os.makedirs(subdirectory, exist_ok=True)\n",
        "\n",
        "    # Construct the full file path\n",
        "    file_path = os.path.join(subdirectory, f'network_radius_{output_suffix}.csv')\n",
        "\n",
        "    # Determine the file mode based on header_written\n",
        "    file_mode = 'w' if not header_written else 'a'\n",
        "\n",
        "    selected_row = df[df['City_lower'] == normalize_string(city)].iloc[0]\n",
        "    data_to_write = [\n",
        "        city,\n",
        "        *city_data,\n",
        "        selected_row['Population 2010'],\n",
        "        selected_row['DENS HAB KM2'],\n",
        "        selected_row['Income 2010'],\n",
        "        selected_row['GDP 2010'],\n",
        "        selected_row['GDP 2017'],\n",
        "        selected_row['IDHM 2010'],\n",
        "        selected_row['Comp Total Ruas'],\n",
        "        selected_row['Number Of Deaths By Traffic Accident'],\n",
        "        selected_row['surfaceOfAdministrativeArea km2']\n",
        "    ]\n",
        "\n",
        "    with open(file_path, file_mode, newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if not header_written:\n",
        "                writer.writerow([\n",
        "                    'City', 'Network Radius',\n",
        "                    'Total Users',\n",
        "                    'Users after Filter 1', 'Percentage Filtered 1 Users',\n",
        "                    'Significant Users', 'Percentage Significant Users',\n",
        "                    'Final Users', 'Percentage Final Users',\n",
        "                    'Total Calls Weight', 'Final Calls Weight',\n",
        "                    'Percentage Final Calls Weight',\n",
        "                    'Records Callee Missed Home Location',\n",
        "                    'Zero Radius Users', 'Percentage Zero Radius Users',\n",
        "                    'Spam Calls', 'Percentage Spam Calls',\n",
        "                    'Number of Antennas',\n",
        "\n",
        "                    'Population 2010', 'DENS HAB KM2',\n",
        "                    'Income 2010', 'GDP 2010',\n",
        "                    'GDP 2017', 'IDHM 2010',\n",
        "                    'Comp Total Ruas', 'Number Of Deaths By Traffic Accident',\n",
        "                    'surfaceOfAdministrativeArea km2'\n",
        "                ])\n",
        "        writer.writerow(data_to_write)\n",
        "    print(f\"Finish writing radius of {city}\")"
      ],
      "metadata": {
        "id": "WmlE3haxnjj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_radius_calculation(calls_between_users, home_locations, antenna_locations, is_log=False):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on the calls between users and their home locations\n",
        "    \"\"\"\n",
        "    # Initialize\n",
        "    total_users = 0\n",
        "    total_distance_weighted = 0.0\n",
        "    total_weight_final = 0.0\n",
        "    callee_miss_home = 0\n",
        "\n",
        "    # Calculate social network distances\n",
        "    social_network = {}\n",
        "    for caller, callees in calls_between_users.items():\n",
        "        if caller in home_locations:\n",
        "            caller_home = home_locations[caller][0]\n",
        "            if caller_home in antenna_locations:\n",
        "                lat1, lon1 = antenna_locations[caller_home]\n",
        "                for callee, call_weight in callees.items():\n",
        "                    if callee in home_locations:\n",
        "                        callee_home = home_locations[callee][0]\n",
        "                        if callee_home in antenna_locations:\n",
        "                            lat2, lon2 = antenna_locations[callee_home]\n",
        "                            distance = haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "                            # Apply log transform to call_weight if is_log is True\n",
        "                            if is_log:\n",
        "                                call_weight = log1p(call_weight)\n",
        "\n",
        "                            # Update social network distances\n",
        "                            if caller not in social_network:\n",
        "                                social_network[caller] = []\n",
        "                            social_network[caller].append(distance * call_weight)\n",
        "                            total_weight_final += call_weight\n",
        "                    else:\n",
        "                        callee_miss_home += 1\n",
        "\n",
        "\n",
        "    # Calculate Final Users\n",
        "    total_users_final = len(social_network)\n",
        "\n",
        "    # Compute network radius\n",
        "    total_distance_weighted = sum([sum(distances) for distances in social_network.values()])\n",
        "    network_radius = total_distance_weighted / total_users_final if total_users_final else 0\n",
        "\n",
        "    del social_network\n",
        "    gc.collect()\n",
        "\n",
        "    return network_radius, total_weight_final, total_users_final, callee_miss_home"
      ],
      "metadata": {
        "id": "1S39qMXV_IqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_radius_calculation_individual(calls_between_users, home_locations, antenna_locations, city, output_suffix, is_log=False, non_zero=False):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on individual callers' network radii and then average.\n",
        "\n",
        "    Args:\n",
        "        calls_between_users: Dictionary with callers and their callees.\n",
        "        home_locations: Dictionary mapping users to their home location (cell).\n",
        "        antenna_locations: Dictionary mapping cell IDs to (latitude, longitude).\n",
        "        city: The name of the city for saving plots.\n",
        "        is_log: Whether to apply a log transform to the call weights.\n",
        "\n",
        "    Returns:\n",
        "        Average network radius for the city, total weight, number of final users, and number of users with missing home locations.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    individual_radii = []\n",
        "    total_weight_final = 0.0\n",
        "    callee_miss_home_record = 0\n",
        "    zero_radius_count = 0\n",
        "\n",
        "    # FOR Distribution analysis\n",
        "    individual_radii_nozero=[]\n",
        "\n",
        "    # Calculate network radius for each caller\n",
        "    for caller, callees in calls_between_users.items():\n",
        "        if caller in home_locations:\n",
        "            caller_home = home_locations[caller][0]\n",
        "            if caller_home in antenna_locations:\n",
        "                lat1, lon1 = antenna_locations[caller_home]\n",
        "                distances = []\n",
        "                total_weight = 0\n",
        "                distances_nozero = []\n",
        "\n",
        "                for callee, call_weight in callees.items():\n",
        "                    if callee in home_locations:\n",
        "                        callee_home = home_locations[callee][0]\n",
        "                        if callee_home in antenna_locations:\n",
        "                            lat2, lon2 = antenna_locations[callee_home]\n",
        "                            distance = haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "                            if distance == 0:\n",
        "                                # distance = 0.5\n",
        "\n",
        "                                # FOR Distribution analysis\n",
        "                                distance_nozero = 0.5\n",
        "                            else:\n",
        "                                distance_nozero = distance\n",
        "\n",
        "                            # Calculate weighted distance\n",
        "                            distances.append(distance * call_weight)\n",
        "\n",
        "                            # FOR Distribution analysis\n",
        "                            distances_nozero.append(distance_nozero * call_weight)\n",
        "\n",
        "                            total_weight += call_weight\n",
        "                    else:\n",
        "                        # callee_miss_home contains duplicated callees as it is actually call records of missing callees\n",
        "                        callee_miss_home_record += 1\n",
        "\n",
        "                # Calculate the individual's network radius\n",
        "                if total_weight > 0:\n",
        "                    individual_radius = sum(distances) / len(callees)\n",
        "\n",
        "                    # FOR Distribution analysis\n",
        "                    individual_radius_nozero = sum(distances_nozero) / len(callees)\n",
        "                    individual_radii_nozero.append(individual_radius_nozero)\n",
        "\n",
        "                    if is_log:\n",
        "                        individual_radius = np.log1p(individual_radius)\n",
        "\n",
        "                    if individual_radius == 0:\n",
        "                        zero_radius_count += 1\n",
        "                        if not non_zero:  # Only append if non_zero is False\n",
        "                            individual_radii.append(individual_radius)\n",
        "                    else:\n",
        "                        individual_radii.append(individual_radius)\n",
        "                    total_weight_final += total_weight\n",
        "\n",
        "    # Calculate the average network radius\n",
        "    network_radius = np.mean(individual_radii) if individual_radii else 0\n",
        "    total_users_final = len(individual_radii)\n",
        "\n",
        "    # Plot the distribution of individual network radii\n",
        "    plot_network_radius_distribution(individual_radii, individual_radii_nozero, city, output_suffix)\n",
        "\n",
        "    return network_radius, total_weight_final, total_users_final, callee_miss_home_record, zero_radius_count"
      ],
      "metadata": {
        "id": "gHei6i597ocX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_network_radius_distribution(individual_radii, individual_radii_nozero, city, output_suffix):\n",
        "    \"\"\"\n",
        "    Plot the distribution of individual network radii and save the plot.\n",
        "\n",
        "    Args:\n",
        "        individual_radii: List of individual network radii.\n",
        "        individual_radii_nozero: List of individual network radii excluding zero values.\n",
        "        city: The name of the city for saving plots.\n",
        "        output_suffix: Suffix for the output file names.\n",
        "    \"\"\"\n",
        "    main_directory = '/content/drive/MyDrive/network_radius_city_size'\n",
        "    subdirectory_1 = os.path.join(main_directory, city)\n",
        "    os.makedirs(subdirectory_1, exist_ok=True)\n",
        "\n",
        "    subdirectory = os.path.join(subdirectory_1, output_suffix)\n",
        "    os.makedirs(subdirectory, exist_ok=True)\n",
        "\n",
        "    # Construct the file paths\n",
        "    hist_file_path = os.path.join(subdirectory, f'network_radius_distribution_{city}_{output_suffix}.png')\n",
        "    log_hist_file_path = os.path.join(subdirectory, f'log_network_radius_distribution_{city}_{output_suffix}.png')\n",
        "    qq_file_path = os.path.join(subdirectory, f'qq_plot_{city}_{output_suffix}.png')\n",
        "\n",
        "    hist_file_path_nozero = os.path.join(subdirectory, f'network_radius_distribution_nozero_{city}_{output_suffix}.png')\n",
        "    log_hist_file_path_nozero = os.path.join(subdirectory, f'log_network_radius_distribution_nozero_{city}_{output_suffix}.png')\n",
        "    qq_file_path_nozero = os.path.join(subdirectory, f'qq_plot_nozero_{city}_{output_suffix}.png')\n",
        "\n",
        "    sqrt_hist_file_path = os.path.join(subdirectory, f'sqrt_network_radius_distribution_{city}_{output_suffix}.png')\n",
        "    sqrt_hist_file_path_nozero = os.path.join(subdirectory, f'sqrt_network_radius_distribution_nozero_{city}_{output_suffix}.png')\n",
        "    sqrt_qq_file_path = os.path.join(subdirectory, f'sqrt_qq_plot_{city}_{output_suffix}.png')\n",
        "    sqrt_qq_file_path_nozero = os.path.join(subdirectory, f'sqrt_qq_plot_nozero_{city}_{output_suffix}.png')\n",
        "\n",
        "    # Plot the distribution for non-zero values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(individual_radii_nozero, bins=30, color='blue', alpha=0.7)\n",
        "    plt.title(f'Network Radius Distribution for {city} (Non-Zero Values)')\n",
        "    plt.xlabel('Network Radius (km)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary\n",
        "    desc_stats = pd.Series(individual_radii_nozero).describe()\n",
        "    stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Statistical Summary:\\n{stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(hist_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the log-transformed distribution for non-zero values\n",
        "    log_radii_nozero = np.log1p(individual_radii_nozero)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(log_radii_nozero, bins=30, color='green', alpha=0.7)\n",
        "    plt.title(f'Log-Transformed Network Radius Distribution for {city} (Non-Zero Values)')\n",
        "    plt.xlabel('Log(Network Radius + 1)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary for log data\n",
        "    log_desc_stats = pd.Series(log_radii_nozero).describe()\n",
        "    log_stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in log_desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Log Statistical Summary:\\n{log_stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(log_hist_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Q-Q plot for non-zero values\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(log_radii_nozero, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Q-Q Plot of Log-Transformed Network Radius for {city} (Non-Zero Values)')\n",
        "\n",
        "    # Save the Q-Q plot\n",
        "    plt.savefig(qq_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the square root-transformed distribution for non-zero values\n",
        "    sqrt_radii_nozero = np.sqrt(individual_radii_nozero)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(sqrt_radii_nozero, bins=30, color='purple', alpha=0.7)\n",
        "    plt.title(f'Square Root-Transformed Network Radius Distribution for {city} (Non-Zero Values)')\n",
        "    plt.xlabel('Square Root(Network Radius)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary for sqrt data\n",
        "    sqrt_desc_stats = pd.Series(sqrt_radii_nozero).describe()\n",
        "    sqrt_stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in sqrt_desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Sqrt Statistical Summary:\\n{sqrt_stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(sqrt_hist_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Q-Q plot for sqrt-transformed distribution\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(sqrt_radii_nozero, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Q-Q Plot of Sqrt-Transformed Network Radius for {city} (Non-Zero Values)')\n",
        "\n",
        "    # Save the Q-Q plot\n",
        "    plt.savefig(sqrt_qq_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(individual_radii, bins=30, color='blue', alpha=0.7)\n",
        "    plt.title(f'Network Radius Distribution for {city}')\n",
        "    plt.xlabel('Network Radius (km)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary\n",
        "    desc_stats = pd.Series(individual_radii).describe()\n",
        "    stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Statistical Summary:\\n{stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(hist_file_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the log-transformed distribution\n",
        "    log_radii = np.log1p(individual_radii)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(log_radii, bins=30, color='green', alpha=0.7)\n",
        "    plt.title(f'Log-Transformed Network Radius Distribution for {city}')\n",
        "    plt.xlabel('Log(Network Radius + 1)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary for log data\n",
        "    log_desc_stats = pd.Series(log_radii).describe()\n",
        "    log_stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in log_desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Log Statistical Summary:\\n{log_stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(log_hist_file_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Q-Q plot\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(log_radii, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Q-Q Plot of Log-Transformed Network Radius for {city}')\n",
        "\n",
        "    # Save the Q-Q plot\n",
        "    plt.savefig(qq_file_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the square root-transformed distribution\n",
        "    sqrt_radii = np.sqrt(individual_radii)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(sqrt_radii, bins=30, color='purple', alpha=0.7)\n",
        "    plt.title(f'Square Root-Transformed Network Radius Distribution for {city}')\n",
        "    plt.xlabel('Square Root(Network Radius)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary for sqrt data\n",
        "    sqrt_desc_stats = pd.Series(sqrt_radii).describe()\n",
        "    sqrt_stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in sqrt_desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Sqrt Statistical Summary:\\n{sqrt_stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(sqrt_hist_file_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Q-Q plot for sqrt-transformed distribution\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(sqrt_radii, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Q-Q Plot of Sqrt-Transformed Network Radius for {city}')\n",
        "\n",
        "    # Save the Q-Q plot\n",
        "    plt.savefig(sqrt_qq_file_path)\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "2NyER5yK7jXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_dict_to_df_count(calls_dict):\n",
        "    # Prepare empty lists to fill with data\n",
        "    data = {'caller_id': [], 'id2': [], 'call_weight': []}\n",
        "\n",
        "    for caller_id, cells in calls_dict.items():\n",
        "        for id2, weight in cells.items():\n",
        "            data['caller_id'].append(caller_id)\n",
        "            data['id2'].append(id2)\n",
        "            data['call_weight'].append(weight)\n",
        "\n",
        "    # Create DataFrame from the collected data\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Convert data types to save memory\n",
        "    df['caller_id']  = df['caller_id'].astype('category')   # Reduces memory by using categorical data type for repetitive strings\n",
        "    df['id2']   = df['id2'].astype('category')        # Similar to caller_id, using category data type for cell ids\n",
        "    df['call_weight'] = df['call_weight'].astype('float32')   # Using float32 for call weights to accommodate decimal values\n",
        "\n",
        "    # Aggregate call weights by caller_id\n",
        "    df_summed = df.groupby('caller_id')['call_weight'].sum().reset_index()\n",
        "    df_summed.columns = ['caller_id', 'total_call_weight']\n",
        "\n",
        "    return df_summed"
      ],
      "metadata": {
        "id": "F5TQo-D6TMND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_dict_to_df_dura(calls_dict):\n",
        "    # Prepare empty lists to fill with data\n",
        "    data = {'caller_id': [], 'callee_id': [], 'total_duration': []}\n",
        "\n",
        "    # Aggregate durations by caller_id to callee_id\n",
        "    for caller_id, callees in calls_dict.items():\n",
        "        for callee_id, duration in callees.items():\n",
        "            data['caller_id'].append(caller_id)\n",
        "            data['callee_id'].append(callee_id)\n",
        "            data['total_duration'].append(duration)\n",
        "\n",
        "    # Create DataFrame from the collected data\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Convert data types to save memory and reflect correct types\n",
        "    df['caller_id'] = df['caller_id'].astype('category')  # Reduces memory for repetitive strings\n",
        "    df['callee_id'] = df['callee_id'].astype('category')  # Same as caller_id\n",
        "    df['total_duration'] = df['total_duration'].astype('float32')  # Ensures duration is in float format\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "6LIdWHIp1yNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions for using counts as the weight"
      ],
      "metadata": {
        "id": "gkVUHMNSJGeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_cdr_count(file_path_cdr):\n",
        "    \"\"\"\n",
        "    Read the CDR file and return a dictionary of user call records,\n",
        "    significant user call records based on time conditions,\n",
        "    and calls between users.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to hold significant user call records based on time conditions to calculate the home location\n",
        "    calls_users_time_filter = {}\n",
        "    # Dictionary to store the calls between users' ids\n",
        "    calls_between_users = {}\n",
        "    # Spam call possibility\n",
        "    dura_spam = 0\n",
        "\n",
        "\n",
        "    # Read the data file line by line\n",
        "    with open(file_path_cdr, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into parts\n",
        "            parts = line.strip().split(';')\n",
        "\n",
        "            duration = float(parts[2])\n",
        "            if duration <= 0.1:\n",
        "                dura_spam += 1\n",
        "                continue\n",
        "\n",
        "            # Extract the columns of interest: Date, Time, Caller ID, Cell ID\n",
        "            date_time_str = parts[0] + ' ' + parts[1]\n",
        "            caller_id = parts[4]\n",
        "            callee_id = parts[6]\n",
        "            cell_id = parts[7]\n",
        "            # Parse date and time\n",
        "            date_time = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
        "            weekday = date_time.weekday()\n",
        "            hour = date_time.hour\n",
        "\n",
        "\n",
        "\n",
        "            # Store calls between users\n",
        "            if caller_id not in calls_between_users:\n",
        "                calls_between_users[caller_id] = {}\n",
        "            else:\n",
        "                if callee_id not in calls_between_users[caller_id]:\n",
        "                    calls_between_users[caller_id][callee_id] = 1\n",
        "                else:\n",
        "                    calls_between_users[caller_id][callee_id] += 1\n",
        "\n",
        "            # Filter based on time conditions\n",
        "            if (weekday < 5 and (hour >= 19 or hour <= 6)) or (weekday >= 5):\n",
        "                if caller_id not in calls_users_time_filter:\n",
        "                    calls_users_time_filter[caller_id] = {}\n",
        "\n",
        "                # Count calls per location for significant times\n",
        "                if cell_id not in calls_users_time_filter[caller_id]:\n",
        "                    calls_users_time_filter[caller_id][cell_id] = 1\n",
        "                else:\n",
        "                    calls_users_time_filter[caller_id][cell_id] += 1\n",
        "    return calls_users_time_filter, calls_between_users, dura_spam"
      ],
      "metadata": {
        "id": "3RgJvamS5Shk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def significant_users_calculation(calls_users_time_filter):\n",
        "    \"\"\"\n",
        "    Calculate the significant users based on the filtered records.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    significant_users = set()\n",
        "\n",
        "    # Determine the significant users based on the filtered records\n",
        "    for user, locations in calls_users_time_filter.items():\n",
        "        # if user not in excluded_users and 5 <= sum(locations.values()) <= 200:\n",
        "        #     significant_users.add(user)\n",
        "        if 5 <= sum(locations.values()) <= 200:\n",
        "            significant_users.add(user)\n",
        "    return significant_users"
      ],
      "metadata": {
        "id": "WydkvgsZ9EIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions for using duration as the weight"
      ],
      "metadata": {
        "id": "_yAHMrWGH4S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_cdr_duration(file_path_cdr):\n",
        "    \"\"\"\n",
        "    Read the CDR file and return a dictionary of user call records,\n",
        "    significant user call records based on time conditions,\n",
        "    and calls between users.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary for holding significant user call records based on time conditions to calculate the home location\n",
        "    calls_users_time_filter = {}\n",
        "    # Dictionary for accumulating duration from callers to the same callees to calculate the social network radius\n",
        "    calls_users_duration = {}\n",
        "    # Spam possibility\n",
        "    dura_spam = 0\n",
        "\n",
        "    # Read the data file line by line\n",
        "    with open(file_path_cdr, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split lines into parts\n",
        "            parts = line.strip().split(';')\n",
        "\n",
        "            # Filter out spam calls\n",
        "            duration = float(parts[2])\n",
        "            if duration <= 0.1:\n",
        "                dura_spam += 1\n",
        "                continue\n",
        "\n",
        "            # Extract columns of interest: Date, Time, Duration, call id, cell id\n",
        "            data_time_str = parts[0] + ' ' + parts[1]\n",
        "            caller_id = parts[4]\n",
        "            callee_id = parts[6]\n",
        "            cell_id = parts[7]\n",
        "\n",
        "            # Parse date and time\n",
        "            date_time = datetime.datetime.strptime(data_time_str, '%Y-%m-%d %H:%M:%S')\n",
        "            weekday = date_time.weekday()\n",
        "            hour = date_time.hour\n",
        "\n",
        "            # Filter based on time conditions for significant users determination\n",
        "            if (weekday < 5 and (hour >= 19 or hour <= 6)) or (weekday >= 5):\n",
        "                if caller_id not in calls_users_time_filter:\n",
        "                    calls_users_time_filter[caller_id] = {}\n",
        "\n",
        "                # Count calls per location for significant times\n",
        "                if cell_id not in calls_users_time_filter[caller_id]:\n",
        "                    calls_users_time_filter[caller_id][cell_id] = 1\n",
        "                else:\n",
        "                    calls_users_time_filter[caller_id][cell_id] += 1\n",
        "\n",
        "            # Accumulate duration for callers to callees\n",
        "            if caller_id not in calls_users_duration:\n",
        "                calls_users_duration[caller_id] = {}\n",
        "            else:\n",
        "                if callee_id not in calls_users_duration[caller_id]:\n",
        "                    calls_users_duration[caller_id][callee_id] = float(duration)\n",
        "                else:\n",
        "                    calls_users_duration[caller_id][callee_id] += float(duration)\n",
        "\n",
        "    return calls_users_time_filter, calls_users_duration, dura_spam\n"
      ],
      "metadata": {
        "id": "Ji9_6GMH7Dis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions for using actual location"
      ],
      "metadata": {
        "id": "40xt9n05afb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_cdr_count_cell(file_path_cdr):\n",
        "    \"\"\"\n",
        "    Read the CDR file and return a dictionary of user call records,\n",
        "    significant user call records based on time conditions,\n",
        "    and calls between users.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to hold significant user call records based on time conditions to calculate the home location\n",
        "    calls_users_time_filter = {}\n",
        "    # List to store the call record\n",
        "    calls_between_users = []\n",
        "    # Spam call possibility\n",
        "    dura_spam = 0\n",
        "\n",
        "\n",
        "    # Read the data file line by line\n",
        "    with open(file_path_cdr, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into parts\n",
        "            parts = line.strip().split(';')\n",
        "\n",
        "            duration = float(parts[2])\n",
        "            if duration <= 0.1:\n",
        "                dura_spam += 1\n",
        "                continue\n",
        "\n",
        "            # Extract the columns of interest: Date, Time, Caller ID, Cell ID\n",
        "            date_time_str = parts[0] + ' ' + parts[1]\n",
        "            caller_id = parts[4]\n",
        "            callee_id = parts[6]\n",
        "            cell_id = parts[7]\n",
        "\n",
        "            # Parse date and time\n",
        "            date_time = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
        "            weekday = date_time.weekday()\n",
        "            hour = date_time.hour\n",
        "\n",
        "            # Store calls between users\n",
        "            calls_between_users.append({\n",
        "                'caller_id': caller_id,\n",
        "                'callee_id': callee_id,\n",
        "                'cell_id': cell_id,\n",
        "            })\n",
        "\n",
        "            # Filter based on time conditions\n",
        "            if (weekday < 5 and (hour >= 19 or hour <= 6)) or (weekday >= 5):\n",
        "                if caller_id not in calls_users_time_filter:\n",
        "                    calls_users_time_filter[caller_id] = {}\n",
        "\n",
        "                # Count calls per location for significant times\n",
        "                if cell_id not in calls_users_time_filter[caller_id]:\n",
        "                    calls_users_time_filter[caller_id][cell_id] = 1\n",
        "                else:\n",
        "                    calls_users_time_filter[caller_id][cell_id] += 1\n",
        "    return calls_users_time_filter, calls_between_users, dura_spam"
      ],
      "metadata": {
        "id": "exwo6BcGaj2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_radius_calculation_cell(calls_between_users, home_locations, antenna_locations, city, output_suffix, is_log=False, non_zero=False):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on the calls between users using the caller's cell location\n",
        "    and the callee's home location, then average. Individually calculate the network radius for each caller.\n",
        "\n",
        "    Args:\n",
        "        calls_between_users: List of call records with caller_id, callee_id, and cell_id.\n",
        "        home_locations: Dictionary mapping users to their home location (cell).\n",
        "        antenna_locations: Dictionary mapping cell IDs to (latitude, longitude).\n",
        "        city: The name of the city for saving plots.\n",
        "        is_log: Whether to apply a log transform to the call weights.\n",
        "\n",
        "    Returns:\n",
        "        Average network radius for the city, total weight, number of final users, and number of users with missing home locations.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    individual_radii = []\n",
        "    total_weight_final = 0.0\n",
        "    callee_miss_home_record = 0\n",
        "    zero_radius_count = 0\n",
        "\n",
        "    # Create a dictionary to hold distances and weights per caller\n",
        "    caller_distances = {}\n",
        "\n",
        "    # FOR Distribution analysis\n",
        "    individual_radii_nozero = []\n",
        "\n",
        "    # Calculate network radius for each caller\n",
        "    for call_record in calls_between_users:\n",
        "        caller_id = call_record['caller_id']\n",
        "        callee_id = call_record['callee_id']\n",
        "        caller_cell_id = call_record['cell_id']\n",
        "\n",
        "        # Get caller's location using the cell ID for each call record\n",
        "        caller_location = antenna_locations.get(caller_cell_id, (np.nan, np.nan))\n",
        "\n",
        "        # Get callee's home location if available\n",
        "        callee_homes = home_locations.get(callee_id, None)\n",
        "\n",
        "        if callee_homes is not None:\n",
        "            callee_location = antenna_locations.get(callee_homes[0], (np.nan, np.nan))\n",
        "\n",
        "            # Proceed if both locations are valid\n",
        "            if not np.isnan(caller_location[0]) and not np.isnan(caller_location[1]) and \\\n",
        "               not np.isnan(callee_location[0]) and not np.isnan(callee_location[1]):\n",
        "\n",
        "                distance = haversine(*caller_location, *callee_location)\n",
        "\n",
        "                if distance == 0:\n",
        "                    # distance = 0.5\n",
        "\n",
        "                    # FOR Distribution analysis\n",
        "                    distance_nozero = 0.5\n",
        "                else:\n",
        "                    distance_nozero = distance\n",
        "\n",
        "                # Initialize call weight\n",
        "                call_weight = 1  # Assuming each call record is weighted equally\n",
        "\n",
        "                # Add distance and weight to caller's record\n",
        "                if caller_id not in caller_distances:\n",
        "                    caller_distances[caller_id] = {'distances': [], 'weights': [], 'distances_nozero': []}\n",
        "                caller_distances[caller_id]['distances'].append(distance * call_weight)\n",
        "                caller_distances[caller_id]['weights'].append(call_weight)\n",
        "\n",
        "                # FOR Distribution analysis\n",
        "                caller_distances[caller_id]['distances_nozero'].append(distance_nozero * call_weight)\n",
        "        else:\n",
        "            callee_miss_home_record += 1\n",
        "\n",
        "    # Calculate the network radius for each caller\n",
        "    for caller_id, data in caller_distances.items():\n",
        "        total_weight = sum(data['weights'])\n",
        "        if total_weight > 0:\n",
        "            individual_radius = sum(data['distances']) / total_weight\n",
        "\n",
        "            # FOR Distribution analysis\n",
        "            individual_radius_nozero = sum(data['distances_nozero']) / total_weight\n",
        "            individual_radii_nozero.append(individual_radius_nozero)\n",
        "\n",
        "            if is_log:\n",
        "                individual_radius = np.log1p(individual_radius)\n",
        "\n",
        "            if individual_radius == 0:\n",
        "                zero_radius_count += 1\n",
        "                if not non_zero:  # Only append if non_zero is False\n",
        "                    individual_radii.append(individual_radius)\n",
        "            else:\n",
        "                individual_radii.append(individual_radius)\n",
        "            total_weight_final += total_weight\n",
        "\n",
        "    # Calculate the average network radius\n",
        "    network_radius = np.mean(individual_radii) if individual_radii else 0\n",
        "    total_users_final = len(individual_radii)\n",
        "\n",
        "    # Plot the distribution of individual network radii\n",
        "    plot_network_radius_distribution(individual_radii, individual_radii_nozero, city, output_suffix)\n",
        "\n",
        "    return network_radius, total_weight_final, total_users_final, callee_miss_home_record, zero_radius_count"
      ],
      "metadata": {
        "id": "z_KDV4RTs0wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions for using duration as the weight and actual location"
      ],
      "metadata": {
        "id": "MsZRhaKq7vl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_cdr_count_cell_dura(file_path_cdr):\n",
        "    \"\"\"\n",
        "    Read the CDR file and return a dictionary of user call records,\n",
        "    significant user call records based on time conditions,\n",
        "    and calls between users.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to hold significant user call records based on time conditions to calculate the home location\n",
        "    calls_users_time_filter = {}\n",
        "    # List to store the call record\n",
        "    calls_between_users = []\n",
        "    # Spam call possibility\n",
        "    dura_spam = 0\n",
        "\n",
        "    # Read the data file line by line\n",
        "    with open(file_path_cdr, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into parts\n",
        "            parts = line.strip().split(';')\n",
        "\n",
        "            duration = float(parts[2])\n",
        "            if duration <= 0.1:\n",
        "                dura_spam += 1\n",
        "                continue\n",
        "\n",
        "            # Extract the columns of interest: Date, Time, Caller ID, Cell ID\n",
        "            date_time_str = parts[0] + ' ' + parts[1]\n",
        "            caller_id = parts[4]\n",
        "            callee_id = parts[6]\n",
        "            cell_id = parts[7]\n",
        "\n",
        "            # Parse date and time\n",
        "            date_time = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
        "            weekday = date_time.weekday()\n",
        "            hour = date_time.hour\n",
        "\n",
        "            # Store calls between users\n",
        "            calls_between_users.append({\n",
        "                'caller_id': caller_id,\n",
        "                'callee_id': callee_id,\n",
        "                'cell_id': cell_id,\n",
        "                'duration': duration\n",
        "            })\n",
        "\n",
        "            # Filter based on time conditions\n",
        "            if (weekday < 5 and (hour >= 19 or hour <= 6)) or (weekday >= 5):\n",
        "                if caller_id not in calls_users_time_filter:\n",
        "                    calls_users_time_filter[caller_id] = {}\n",
        "\n",
        "                # Count calls per location for significant times\n",
        "                if cell_id not in calls_users_time_filter[caller_id]:\n",
        "                    calls_users_time_filter[caller_id][cell_id] = 1\n",
        "                else:\n",
        "                    calls_users_time_filter[caller_id][cell_id] += 1\n",
        "    return calls_users_time_filter, calls_between_users, dura_spam"
      ],
      "metadata": {
        "id": "Canx3P8N7ss8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_radius_calculation_cell_dura(calls_between_users, home_locations, antenna_locations, city, output_suffix, is_log=False, non_zero=False):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on the calls between users using the caller's cell location\n",
        "    and the callee's home location, then average.\n",
        "\n",
        "    Args:\n",
        "        calls_between_users: List of call records with caller_id, callee_id, and cell_id.\n",
        "        home_locations: Dictionary mapping users to their home location (cell).\n",
        "        antenna_locations: Dictionary mapping cell IDs to (latitude, longitude).\n",
        "        city: The name of the city for saving plots.\n",
        "        is_log: Whether to apply a log transform to the call weights.\n",
        "        non_zero: Whether to exclude zero individual radii from the final calculation.\n",
        "\n",
        "    Returns:\n",
        "        Average network radius for the city, total weight, number of final users, and number of users with missing home locations.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    individual_radii = []\n",
        "    total_weight_final = 0.0\n",
        "    callee_miss_home_record = 0\n",
        "    zero_radius_count = 0\n",
        "\n",
        "    # FOR Distribution analysis\n",
        "    individual_radii_nozero = []\n",
        "\n",
        "    # Create a dictionary to hold distances and weights per caller\n",
        "    caller_distances = {}\n",
        "\n",
        "    # Calculate network radius for each caller\n",
        "    for call_record in calls_between_users:\n",
        "        caller_id = call_record['caller_id']\n",
        "        callee_id = call_record['callee_id']\n",
        "        caller_cell_id = call_record['cell_id']\n",
        "        call_weight = call_record['duration']\n",
        "\n",
        "        # Get caller's location using the cell ID for each call record\n",
        "        caller_location = antenna_locations.get(caller_cell_id, (np.nan, np.nan))\n",
        "\n",
        "        # Get callee's home location if available\n",
        "        callee_homes = home_locations.get(callee_id, None)\n",
        "\n",
        "        if callee_homes is not None:\n",
        "            callee_location = antenna_locations.get(callee_homes[0], (np.nan, np.nan))\n",
        "\n",
        "            # Proceed if both locations are valid\n",
        "            if not np.isnan(caller_location[0]) and not np.isnan(caller_location[1]) and \\\n",
        "               not np.isnan(callee_location[0]) and not np.isnan(callee_location[1]):\n",
        "\n",
        "                distance = haversine(*caller_location, *callee_location)\n",
        "\n",
        "                if distance == 0:\n",
        "                    # distance = 0.5\n",
        "\n",
        "                    # FOR Distribution analysis\n",
        "                    distance_nozero = 0.5\n",
        "                else:\n",
        "                    distance_nozero = distance\n",
        "\n",
        "                # Add distance and weight to caller's record\n",
        "                if caller_id not in caller_distances:\n",
        "                    caller_distances[caller_id] = {'distances': [], 'weights': [], 'distances_nozero': []}\n",
        "                caller_distances[caller_id]['distances'].append(distance * call_weight)\n",
        "                caller_distances[caller_id]['weights'].append(call_weight)\n",
        "\n",
        "                # FOR Distribution analysis\n",
        "                caller_distances[caller_id]['distances_nozero'].append(distance_nozero * call_weight)\n",
        "\n",
        "        else:\n",
        "            callee_miss_home_record += 1\n",
        "\n",
        "    # Calculate the network radius for each caller\n",
        "    for caller_id, data in caller_distances.items():\n",
        "        total_weight = sum(data['weights'])\n",
        "        if total_weight > 0:\n",
        "            individual_radius = sum(data['distances']) / total_weight\n",
        "\n",
        "            # FOR Distribution analysis\n",
        "            individual_radius_nozero = sum(data['distances_nozero']) / total_weight\n",
        "            individual_radii_nozero.append(individual_radius_nozero)\n",
        "\n",
        "            if is_log:\n",
        "                individual_radius = np.log1p(individual_radius)\n",
        "\n",
        "            if individual_radius == 0:\n",
        "                zero_radius_count += 1\n",
        "                if not non_zero:  # Only append if non_zero is False\n",
        "                    individual_radii.append(individual_radius)\n",
        "            else:\n",
        "                individual_radii.append(individual_radius)\n",
        "\n",
        "            total_weight_final += total_weight\n",
        "\n",
        "    # Calculate the average network radius\n",
        "    network_radius = np.mean(individual_radii) if individual_radii else 0\n",
        "    total_users_final = len(individual_radii)\n",
        "\n",
        "    # Plot the distribution of individual network radii\n",
        "    plot_network_radius_distribution(individual_radii, individual_radii_nozero, city, output_suffix)\n",
        "\n",
        "    return network_radius, total_weight_final, total_users_final, callee_miss_home_record, zero_radius_count"
      ],
      "metadata": {
        "id": "tVgvwyRK8nbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1 to calculate the radius\n",
        "\n",
        "\n",
        "\n",
        "1.   Home location for both callers and callees;\n",
        "2.   Call counts as the weight.\n",
        "\n"
      ],
      "metadata": {
        "id": "zU7sbsLGGuMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribution analysis for weight(distance here)"
      ],
      "metadata": {
        "id": "6wyUPb2dGXYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    # excluded_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_between_users = read_cdr_count(file_path_cdr)\n",
        "\n",
        "    df_calls = convert_dict_to_df_count(calls_users_time_filter)\n",
        "\n",
        "    # q_low = df_calls['total_call_weight'].quantile(0.1)\n",
        "    # q_high = df_calls['total_call_weight'].quantile(0.9)\n",
        "    # q_low = 5\n",
        "    # q_high = 200\n",
        "\n",
        "    # print(f'q_low: {q_low}, q_high: {q_high}')\n",
        "\n",
        "    # df_calls = df_calls[(df_calls['total_call_weight'] >= q_low) & (df_calls['total_call_weight'] <= q_high)]\n",
        "\n",
        "    # stat, p = normaltest(df_calls['total_call_weight'])\n",
        "    # normality_result = \"Gaussian (fail to reject H0)\" if p > 0.05 else \"not Gaussian (reject H0)\"\n",
        "    # print(f'Statistics={stat:.3f}, p={p:.3f}')\n",
        "    # if p > 0.05:\n",
        "    #     print(f'{city} looks Gaussian (fail to reject H0)')\n",
        "    # else:\n",
        "    #     print(f'{city} does not look Gaussian (reject H0)')\n",
        "\n",
        "    # Plotting the KDE (Kernel Density Estimate)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if not df_calls['total_call_weight'].empty:\n",
        "        sns.histplot(df_calls['total_call_weight'], bins=30, kde=False)\n",
        "        plt.title(f'Frequency Distribution of Call Counts in {city}')\n",
        "        plt.xlabel('Call Counts')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Calculate and display statistical summary\n",
        "        desc_stats = df_calls['total_call_weight'].describe()\n",
        "        stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in desc_stats.items()])\n",
        "        # stats_text += f'\\nNormality Test: {normality_result} (p={p:.3f})'\n",
        "        plt.figtext(0.35, 0.4, f'Statistical Summary:\\n{stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "        # Save the KDE plot\n",
        "        kde_path = os.path.join('/content/drive/MyDrive/network_radius_city_size/distribution analysis/call counts/all range', f'Call_Counts_{city}.png')\n",
        "        plt.savefig(kde_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(f\"No call weight data available for {city}.\")"
      ],
      "metadata": {
        "id": "fT_H4XrUTmKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Loop of Method 1"
      ],
      "metadata": {
        "id": "g2aSELFeGgwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "is_log = False\n",
        "non_zero = False\n",
        "output_suffix = f\"call_counts_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_home\"\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    # excluded_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_between_users, dura_spam = read_cdr_count(file_path_cdr)\n",
        "\n",
        "    # Determine significant users\n",
        "    significant_users = significant_users_calculation(calls_users_time_filter)\n",
        "\n",
        "    # Determine the home location based on the filtered records\n",
        "    home_locations = home_location_calculation(calls_users_time_filter, significant_users)\n",
        "\n",
        "    count_antenna = len(antenna_locations)\n",
        "    count_significant_users = len(significant_users)\n",
        "    filter_users_1 = len(calls_users_time_filter)\n",
        "    total_users = len(calls_between_users)\n",
        "    percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "    percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "    total_calls_record = sum(sum(calls.values()) for calls in calls_between_users.values()) + dura_spam\n",
        "    percentage_dura_spam = (dura_spam / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "\n",
        "    # release calls_users_time_filter and significant_users\n",
        "    del calls_users_time_filter\n",
        "    del significant_users\n",
        "    gc.collect()\n",
        "\n",
        "    # ** Calculate the network radius method 1**\n",
        "    # network_radius, total_weight_final, total_users_final, callee_miss_home = network_radius_calculation(calls_between_users, home_locations, antenna_locations, is_log = is_log)\n",
        "\n",
        "    # ** Calculation methond 2 **\n",
        "    network_radius, total_weight_final, total_users_final, callee_miss_home, zero_radius_count = network_radius_calculation_individual(\n",
        "        calls_between_users, home_locations, antenna_locations, city, output_suffix, is_log=is_log, non_zero=non_zero)\n",
        "\n",
        "    del calls_between_users\n",
        "    del home_locations\n",
        "    del antenna_locations\n",
        "    gc.collect()\n",
        "\n",
        "    percentage_zero_radius_count = (zero_radius_count / total_users_final) * 100 if total_users_final else 0\n",
        "    percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "    percentage_final_calls = (total_weight_final / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_record,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home,\n",
        "            zero_radius_count,\n",
        "            percentage_zero_radius_count,\n",
        "            dura_spam,\n",
        "            percentage_dura_spam,\n",
        "            count_antenna\n",
        "            )\n",
        "\n",
        "    print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "    write_data_to_csv(df, city, city_data, output_suffix)\n",
        "    header_written = True  # Update the global variable after writing header"
      ],
      "metadata": {
        "id": "6Nx214myfRBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2 to calculate the radius\n",
        "\n",
        "1.   both home location for callers and callees\n",
        "2.   Duration as weight\n"
      ],
      "metadata": {
        "id": "B-EK3pOs7EAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribution analysis for weight(duration here)"
      ],
      "metadata": {
        "id": "cdtxUH1iyFI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    # excluded_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_users_duration = read_cdr_duration(file_path_cdr)\n",
        "\n",
        "    df_calls = convert_dict_to_df_count(calls_users_duration)\n",
        "\n",
        "    # q_low = df_calls['total_call_weight'].quantile(0.1)\n",
        "    # q_high = df_calls['total_call_weight'].quantile(0.9)\n",
        "    # q_low = 5\n",
        "    # q_high = 200\n",
        "\n",
        "    # print(f'q_low: {q_low}, q_high: {q_high}')\n",
        "\n",
        "    # df_calls = df_calls[(df_calls['total_call_weight'] >= q_low) & (df_calls['total_call_weight'] <= q_high)]\n",
        "\n",
        "    stat, p = normaltest(df_calls['total_call_weight'])\n",
        "    normality_result = \"Gaussian (fail to reject H0)\" if p > 0.05 else \"not Gaussian (reject H0)\"\n",
        "    print(f'Statistics={stat:.3f}, p={p:.3f}')\n",
        "    if p > 0.05:\n",
        "        print(f'{city} looks Gaussian (fail to reject H0)')\n",
        "    else:\n",
        "        print(f'{city} does not look Gaussian (reject H0)')\n",
        "\n",
        "    # Plotting the KDE (Kernel Density Estimate)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if not df_calls['total_call_weight'].empty:\n",
        "        sns.histplot(df_calls['total_call_weight'], bins=30, kde=False)\n",
        "        plt.title(f'Frequency Distribution of Call Duration in {city}')\n",
        "        plt.xlabel('Call duration')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Calculate and display statistical summary\n",
        "        desc_stats = df_calls['total_call_weight'].describe()\n",
        "        stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in desc_stats.items()])\n",
        "        # stats_text += f'\\nNormality Test: {normality_result} (p={p:.3f})'\n",
        "        plt.figtext(0.35, 0.4, f'Statistical Summary:\\n{stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "        # Save the KDE plot\n",
        "        kde_path = os.path.join('/content/drive/MyDrive/network_radius_city_size/distribution analysis/call duration/all range', f'Call_duration_{city}.png')\n",
        "        plt.savefig(kde_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(f\"No call weight data available for {city}.\")"
      ],
      "metadata": {
        "id": "SC66ZM5UyERD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Loop for Method 2"
      ],
      "metadata": {
        "id": "eJ_uHjysHwUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "is_log = False\n",
        "non_zero = False\n",
        "output_suffix = f\"dura_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_home\"\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # Check if cdr and ana files are in the folders\n",
        "    if cdr_file_name not in os.listdir(folder_path_cdr):\n",
        "        print(f\"File {cdr_file_name} not found in the folder.\")\n",
        "        continue\n",
        "\n",
        "    if ana_file_name not in os.listdir(folder_path_ana):\n",
        "        print(f\"File {ana_file_name} not found in the folder.\")\n",
        "        continue\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_users_duration, dura_spam = read_cdr_duration(file_path_cdr)\n",
        "\n",
        "    # Determine significant users\n",
        "    significant_users = significant_users_calculation(calls_users_time_filter)\n",
        "\n",
        "    # Determine the home location based on the filtered records\n",
        "    home_locations = home_location_calculation(calls_users_time_filter, significant_users)\n",
        "\n",
        "    count_antenna = len(antenna_locations)\n",
        "    total_users = len(calls_users_duration)\n",
        "    total_calls_weight = sum(sum(calls.values()) for calls in calls_users_duration.values())\n",
        "\n",
        "    filter_users_1 = len(calls_users_time_filter)\n",
        "    percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "\n",
        "    count_significant_users = len(significant_users)\n",
        "    percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "\n",
        "    # release calls_users_time_filter and significant_users\n",
        "    del calls_users_time_filter\n",
        "    del significant_users\n",
        "    gc.collect()\n",
        "\n",
        "    # Calculate the network radius\n",
        "    network_radius, total_weight_final, total_users_final, callee_miss_home, zero_radius_count = network_radius_calculation_individual(\n",
        "        calls_users_duration, home_locations, antenna_locations, city, output_suffix, is_log=is_log, non_zero=non_zero)\n",
        "\n",
        "    del calls_users_duration\n",
        "    del home_locations\n",
        "    del antenna_locations\n",
        "    gc.collect()\n",
        "\n",
        "    percentage_zero_radius_count = (zero_radius_count / total_users_final) * 100 if total_users_final else 0\n",
        "    percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "    percentage_final_calls = (total_weight_final / total_calls_weight) * 100 if total_calls_weight else 0\n",
        "\n",
        "    city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_weight,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home,\n",
        "            zero_radius_count,\n",
        "            percentage_zero_radius_count,\n",
        "            dura_spam,\n",
        "            0,\n",
        "            count_antenna\n",
        "            )\n",
        "\n",
        "    print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "    write_data_to_csv(df, city, city_data, output_suffix)\n",
        "    header_written = True  # Update the global variable after writing header"
      ],
      "metadata": {
        "id": "DV2n2o_8n0S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 3 to calculate the radius\n",
        "\n",
        "1.   Cell location for callers and home location for callees\n",
        "2.   Call counts as weight"
      ],
      "metadata": {
        "id": "FySjkrortT_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "\n",
        "is_log = False\n",
        "non_zero = False\n",
        "output_suffix = f\"call_counts_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_cell\"\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_between_users, dura_spam = read_cdr_count_cell(file_path_cdr)\n",
        "\n",
        "    # Determine significant users\n",
        "    significant_users = significant_users_calculation(calls_users_time_filter)\n",
        "\n",
        "    # Determine the home location based on the filtered records\n",
        "    home_locations = home_location_calculation(calls_users_time_filter, significant_users)\n",
        "\n",
        "    count_antenna = len(antenna_locations)\n",
        "    count_significant_users = len(significant_users)\n",
        "    filter_users_1 = len(calls_users_time_filter)\n",
        "    total_users = len(set(record['caller_id'] for record in calls_between_users))\n",
        "    percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "    percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "    total_calls_record = len(calls_between_users) + dura_spam\n",
        "    percentage_dura_spam = (dura_spam / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    # release calls_users_time_filter and significant_users\n",
        "    del calls_users_time_filter\n",
        "    del significant_users\n",
        "    gc.collect()\n",
        "\n",
        "    network_radius, total_weight_final, total_users_final, callee_miss_home, zero_radius_count = network_radius_calculation_cell(\n",
        "        calls_between_users=calls_between_users, home_locations=home_locations, antenna_locations=antenna_locations,\n",
        "        city=city, output_suffix=output_suffix, is_log=is_log, non_zero=non_zero)\n",
        "\n",
        "    del calls_between_users\n",
        "    del home_locations\n",
        "    del antenna_locations\n",
        "    gc.collect()\n",
        "\n",
        "    percentage_zero_radius_count = (zero_radius_count / total_users_final) * 100 if total_users_final else 0\n",
        "    percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "    percentage_final_calls = (total_weight_final / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_record,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home,\n",
        "            zero_radius_count,\n",
        "            percentage_zero_radius_count,\n",
        "            dura_spam,\n",
        "            percentage_dura_spam,\n",
        "            count_antenna\n",
        "            )\n",
        "\n",
        "    print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "    write_data_to_csv(df, city, city_data, output_suffix)\n",
        "    header_written = True  # Update the global variable after writing header"
      ],
      "metadata": {
        "id": "pXjryIkAtajh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 4 to calculate the radius\n",
        "\n",
        "1.   Cell location for callers and home location for callees\n",
        "2.   Call duration as weight"
      ],
      "metadata": {
        "id": "2WZybNbw-KPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "is_log = False\n",
        "non_zero = False\n",
        "output_suffix = f\"dura_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_cell\"\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_between_users, dura_spam = read_cdr_count_cell_dura(file_path_cdr)\n",
        "\n",
        "    # Determine significant users\n",
        "    significant_users = significant_users_calculation(calls_users_time_filter)\n",
        "\n",
        "    # Determine the home location based on the filtered records\n",
        "    home_locations = home_location_calculation(calls_users_time_filter, significant_users)\n",
        "\n",
        "    count_antenna = len(antenna_locations)\n",
        "    count_significant_users = len(significant_users)\n",
        "    filter_users_1 = len(calls_users_time_filter)\n",
        "    total_users = len(set(record['caller_id'] for record in calls_between_users))\n",
        "    percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "    percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "    total_calls_record = len(calls_between_users) + dura_spam\n",
        "    percentage_dura_spam = (dura_spam / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    # release calls_users_time_filter and significant_users\n",
        "    del calls_users_time_filter\n",
        "    del significant_users\n",
        "    gc.collect()\n",
        "\n",
        "    network_radius, total_weight_final, total_users_final, callee_miss_home, zero_radius_count = network_radius_calculation_cell_dura(\n",
        "        calls_between_users=calls_between_users, home_locations=home_locations, antenna_locations=antenna_locations,\n",
        "        city=city, output_suffix=output_suffix, is_log=is_log, non_zero=non_zero)\n",
        "\n",
        "    # Calculate the network radius\n",
        "    # network_radius, total_weight_final, total_users_final, callee_miss_home = network_radius_calculation_cell(\n",
        "        # calls_between_users, home_locations, antenna_locations, is_log = is_log)\n",
        "\n",
        "    del calls_between_users\n",
        "    del home_locations\n",
        "    del antenna_locations\n",
        "    gc.collect()\n",
        "\n",
        "    percentage_zero_radius_count = (zero_radius_count / total_users_final) * 100 if total_users_final else 0\n",
        "    percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "    percentage_final_calls = (total_weight_final / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_record,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home,\n",
        "            zero_radius_count,\n",
        "            percentage_zero_radius_count,\n",
        "            dura_spam,\n",
        "            percentage_dura_spam,\n",
        "            count_antenna\n",
        "            )\n",
        "\n",
        "    print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "    write_data_to_csv(df, city, city_data, output_suffix)\n",
        "    header_written = True  # Update the global variable after writing header"
      ],
      "metadata": {
        "id": "vsyDg04Y-OSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframe Method"
      ],
      "metadata": {
        "id": "1uq-rsbPr5Py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Utility Functions"
      ],
      "metadata": {
        "id": "iBuIynL6r9ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CDR data into a DataFrame\n",
        "def load_cdr_data(file_path_cdr):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "      file_path_cdr:\n",
        "\n",
        "    Returns:\n",
        "      Dataframe of CDR data\n",
        "\n",
        "    \"\"\"\n",
        "    columns_to_use = [0, 1, 2, 4, 6, 7]\n",
        "    column_names = ['date', 'time', 'duration', 'caller_id', 'callee_id', 'cell_id']\n",
        "\n",
        "    # Specify data types for each column to optimize memory usage\n",
        "    dtype = {\n",
        "        'duration': 'float32',\n",
        "        'caller_id': 'category',\n",
        "        'callee_id': 'category',\n",
        "        'cell_id': 'category'\n",
        "    }\n",
        "\n",
        "    # Read the data including only the required columns and specifying the date-time format\n",
        "    df_cdr = pd.read_csv(\n",
        "        file_path_cdr,\n",
        "        sep=';',\n",
        "        header=None,\n",
        "        usecols=columns_to_use,\n",
        "        dtype=dtype,\n",
        "        names=column_names,\n",
        "        parse_dates={'datetime': ['date', 'time']},\n",
        "        date_format='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "\n",
        "    # Add a 'weekday' and 'hour' column for time filtering\n",
        "    df_cdr['weekday'] = df_cdr['datetime'].dt.weekday\n",
        "    df_cdr['hour'] = df_cdr['datetime'].dt.hour\n",
        "\n",
        "    return df_cdr"
      ],
      "metadata": {
        "id": "c_SuYRM7uLoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for significant time conditions and aggregate call data\n",
        "def filter_significant_calls(df_cdr):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "      df_cdr:\n",
        "\n",
        "    Returns:\n",
        "      List of significant users and dictionary of their home locations\n",
        "\n",
        "    \"\"\"\n",
        "    # Filter based on time conditions (weekdays after 7 PM and before 6 AM, weekends all day)\n",
        "    significant_calls = df_cdr[((df_cdr['weekday'] < 5) & ((df_cdr['hour'] >= 19) | (df_cdr['hour'] <= 6))) | (df_cdr['weekday'] >= 5)]\n",
        "\n",
        "    filter_users_1 = significant_calls['caller_id'].nunique()\n",
        "\n",
        "    # Group by caller and cell_id to get call counts\n",
        "    calls_users_time_filter = significant_calls.groupby(['caller_id', 'cell_id']).size().reset_index(name='call_count')\n",
        "\n",
        "    # Identify significant users\n",
        "    significant_users = calls_users_time_filter.groupby('caller_id')['call_count'].sum()\n",
        "    significant_users = significant_users[(significant_users >= 5) & (significant_users <= 200)].index.tolist()\n",
        "\n",
        "    # Determine home locations based on most frequent cell_id for each significant user\n",
        "    home_locations = calls_users_time_filter[calls_users_time_filter['caller_id'].isin(significant_users)]\n",
        "\n",
        "    home_locations = home_locations.loc[home_locations.groupby('caller_id', observed=True)['call_count'].idxmax()]\n",
        "\n",
        "    # home_locations = home_locations.loc[home_locations.groupby('caller_id')['call_count'].idxmax()]\n",
        "\n",
        "    return home_locations.set_index('caller_id')['cell_id'].to_dict(), significant_users, filter_users_1"
      ],
      "metadata": {
        "id": "RRTuB6SkvWop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Calculate the network radius\n",
        "# def calculate_network_radius(df_cdr, home_locations, antenna_locations, weight='call_count', is_log=False, location='home'):\n",
        "#     \"\"\"\n",
        "#     Calculate the network radius based on calls between users and their home locations\n",
        "\n",
        "#     Args:\n",
        "#       df_cdr: Dataframe of CDR data\n",
        "#       home_locations: Dictionary of significant users and their home locations\n",
        "#       antenna_locations: Dictionary of cells' locations\n",
        "#       weight: 'call_count' or 'duration' to use different value as weight (default 'counts')\n",
        "#       is_log: 'Ture' or 'False' to use log1p transform or not (default 'False')\n",
        "#       location: 'home' or 'cell' to use home locations or actual location of each call record for callers,\n",
        "#             always use home location for callees (default 'home')\n",
        "\n",
        "#     Returns:\n",
        "#       Network radius, total weight, total users, and number of users with missing home locations\n",
        "#     \"\"\"\n",
        "#     total_weight_final = 0.0\n",
        "#     callee_miss_home = 0\n",
        "\n",
        "#     # Simplify dataframe by remove redundant columns\n",
        "#     if location == 'home':\n",
        "#         if weight == 'call_count':\n",
        "#             df_cdr = df_cdr[['caller_id', 'callee_id']]\n",
        "#         else:\n",
        "#             df_cdr = df_cdr[['caller_id', 'callee_id', 'duration']]\n",
        "#     else:\n",
        "#         if weight == 'call_count':\n",
        "#             df_cdr = df_cdr[['caller_id', 'callee_id', 'cell_id']]\n",
        "#         else:\n",
        "#             df_cdr = df_cdr[['caller_id', 'callee_id', 'cell_id', 'duration']]\n",
        "\n",
        "#     # Use vectorized operations and avoid apply when possible\n",
        "#     def get_location(row, type_):\n",
        "#         if type_ == 'caller':\n",
        "#             return home_locations.get(row['caller_id'], (np.nan, np.nan)) if location == 'home' else antenna_locations.get(row['cell_id'], (np.nan, np.nan))\n",
        "#         else:\n",
        "#             return home_locations.get(row['callee_id'], (np.nan, np.nan))\n",
        "\n",
        "#     df_cdr[['caller_lat', 'caller_lon']] = df_cdr.apply(lambda row: pd.Series(get_location(row, 'caller')), axis=1)\n",
        "#     df_cdr[['callee_lat', 'callee_lon']] = df_cdr.apply(lambda row: pd.Series(get_location(row, 'callee')), axis=1)\n",
        "\n",
        "#     df_cdr['distance'] = df_cdr.apply(lambda row: haversine(row['caller_lat'], row['caller_lon'], row['callee_lat'], row['callee_lon']), axis=1)\n",
        "\n",
        "#     # Count the number of unique callees with missing home locations\n",
        "#     callee_miss_home = len(pd.Index(df_cdr['callee_id'].unique()).difference(home_locations.keys()))\n",
        "\n",
        "#     # Filter for rows where both caller and callee have locations\n",
        "#     valid_calls = df_cdr.dropna(subset=['distance'])\n",
        "\n",
        "#     # Compute call_count if required\n",
        "#     if weight == 'call_count':\n",
        "#         # Count occurrences of each (caller_id, callee_id) pair in valid_calls\n",
        "#         call_counts = valid_calls.groupby(['caller_id', 'callee_id']).size().reset_index(name='call_count')\n",
        "#         valid_calls = valid_calls.merge(call_counts, on=['caller_id', 'callee_id'], how='left')\n",
        "\n",
        "#     # Choose the weight column\n",
        "#     weight_column = 'duration' if weight == 'duration' else 'call_count'\n",
        "\n",
        "#     # Apply log transform to the weight if specified\n",
        "#     if is_log:\n",
        "#         valid_calls[weight_column] = np.log1p(valid_calls[weight_column])\n",
        "\n",
        "#     valid_calls['weighted_distance'] = valid_calls['distance'] * valid_calls[weight_column]\n",
        "#     total_distance_weighted = valid_calls['weighted_distance'].sum()\n",
        "#     total_weight_final = valid_calls[weight_column].sum()\n",
        "\n",
        "#     network_radius = total_distance_weighted / valid_calls['caller_id'].nunique() if valid_calls['caller_id'].nunique() else 0\n",
        "\n",
        "#     return network_radius, total_weight_final, valid_calls['caller_id'].nunique(), callee_miss_home"
      ],
      "metadata": {
        "id": "jAvE0NijD09e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the network radius\n",
        "def calculate_network_radius(df_cdr, home_locations, antenna_locations, weight='call_count', is_log=False, location='home'):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on calls between users and their home locations.\n",
        "\n",
        "    Args:\n",
        "      df_cdr: Dataframe of CDR data\n",
        "      home_locations: Dictionary of significant users and their home locations\n",
        "      antenna_locations: Dictionary of cells' locations\n",
        "      weight: 'call_count' or 'duration' to use different value as weight (default 'counts')\n",
        "      is_log: 'True' or 'False' to use log1p transform or not (default 'False')\n",
        "      location: 'home' or 'cell' to use home locations or actual location of each call record for callers,\n",
        "                always use home location for callees (default 'home')\n",
        "\n",
        "    Returns:\n",
        "      Network radius, total weight, total users, and number of users with missing home locations\n",
        "    \"\"\"\n",
        "    total_weight_final = 0.0\n",
        "\n",
        "    # Simplify dataframe by removing redundant columns\n",
        "    if location == 'home':\n",
        "        if weight == 'call_count':\n",
        "            # Group by caller and callee to get call counts, and reduce to unique pairs\n",
        "            call_counts = df_cdr.groupby(['caller_id', 'callee_id']).size().reset_index(name='call_count')\n",
        "            df_cdr = call_counts\n",
        "        else:\n",
        "            df_cdr = df_cdr[['caller_id', 'callee_id', 'duration']]\n",
        "    else:\n",
        "        if weight == 'call_count':\n",
        "            call_counts = df_cdr.groupby(['caller_id', 'callee_id', 'cell_id']).size().reset_index(name='call_count')\n",
        "            df_cdr = call_counts\n",
        "        else:\n",
        "            df_cdr = df_cdr[['caller_id', 'callee_id', 'cell_id', 'duration']]\n",
        "\n",
        "    # Add locations based on home or cell\n",
        "    def get_location(type_, row):\n",
        "        if type_ == 'caller':\n",
        "            return home_locations.get(row['caller_id']) if location == 'home' else antenna_locations.get(row['cell_id'])\n",
        "        else:\n",
        "            return home_locations.get(row['callee_id'])\n",
        "\n",
        "    # Calculate distances and filter valid calls\n",
        "    df_cdr['caller_location'] = df_cdr.apply(lambda row: get_location('caller', row), axis=1)\n",
        "    df_cdr['callee_location'] = df_cdr.apply(lambda row: get_location('callee', row), axis=1)\n",
        "\n",
        "    # Drop rows with missing locations\n",
        "    valid_calls = df_cdr.dropna(subset=['caller_location', 'callee_location'])\n",
        "\n",
        "    # Calculate distance using vectorized operations\n",
        "    valid_calls['distance'] = valid_calls.apply(\n",
        "        lambda row: haversine(*(row['caller_location']), *(row['callee_location'])),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Select weight column\n",
        "    weight_column = 'duration' if weight == 'duration' else 'call_count'\n",
        "\n",
        "    # Apply log transform if specified\n",
        "    if is_log:\n",
        "        valid_calls[weight_column] = np.log1p(valid_calls[weight_column])\n",
        "\n",
        "    # Calculate weighted distances\n",
        "    valid_calls['weighted_distance'] = valid_calls['distance'] * valid_calls[weight_column]\n",
        "    total_distance_weighted = valid_calls['weighted_distance'].sum()\n",
        "    total_weight_final = valid_calls[weight_column].sum()\n",
        "\n",
        "    # Calculate network radius\n",
        "    network_radius = total_distance_weighted / valid_calls['caller_id'].nunique() if valid_calls['caller_id'].nunique() else 0\n",
        "\n",
        "    # Count missing home locations for callees\n",
        "    unique_callees = pd.Series(df_cdr['callee_id'].unique())\n",
        "    callee_miss_home = len(unique_callees) - unique_callees.isin(home_locations.keys()).sum()\n",
        "\n",
        "    return network_radius, total_weight_final, valid_calls['caller_id'].nunique(), callee_miss_home"
      ],
      "metadata": {
        "id": "QEtxHlj3gwaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop to process each city\n",
        "def process_cities(weight, is_log, location, output_suffix):\n",
        "    \"\"\"\n",
        "    Main loop to process each city with given parameters for network radius calculation.\n",
        "\n",
        "    Args:\n",
        "      weight: 'call_count' or 'duration'\n",
        "      is_log: True or False\n",
        "      location: 'home' or 'cell'\n",
        "      output_suffix: Suffix for the output file name to differentiate parameter sets\n",
        "    \"\"\"\n",
        "    # Read city data\n",
        "    file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "    df_city = pd.read_excel(file_path_cidades)\n",
        "    df_city['City_lower'] = df_city['City'].apply(normalize_string)\n",
        "\n",
        "    folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "    folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "    file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "    cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "    header_written = False\n",
        "\n",
        "    for city in cities:\n",
        "        if city == 'Fortaleza':\n",
        "            continue\n",
        "\n",
        "        # Load data for the city\n",
        "        file_path_cdr = os.path.join(folder_path_cdr, f'cdr_{city}.txt')\n",
        "        file_path_ana = os.path.join(folder_path_ana, f'antennas_{city}.txt')\n",
        "        df_cdr = load_cdr_data(file_path_cdr)\n",
        "        antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "        # Process data\n",
        "        home_locations, significant_users, filter_users_1 = filter_significant_calls(df_cdr)\n",
        "        network_radius, total_weight_final, total_users_final, callee_miss_home = calculate_network_radius(\n",
        "            df_cdr, home_locations, antenna_locations, weight=weight, is_log=is_log, location=location\n",
        "        )\n",
        "\n",
        "        # Calculate percentages and other metrics\n",
        "        total_users = df_cdr['caller_id'].nunique()\n",
        "        count_significant_users = len(significant_users)\n",
        "        percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "        percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "        total_calls_record = df_cdr.shape[0]\n",
        "        percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "        percentage_final_calls = (total_weight_final / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "        del df_cdr, antenna_locations\n",
        "        gc.collect()\n",
        "\n",
        "        city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_record,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home\n",
        "        )\n",
        "\n",
        "        print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "        # Write data to CSV\n",
        "        write_data_to_csv(df_city, city, city_data, output_suffix, header_written)\n",
        "        header_written = True"
      ],
      "metadata": {
        "id": "W-fd_AG5NN5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the process for all combinations of parameters\n",
        "weights = ['call_count', 'duration']\n",
        "logs = [False, True]\n",
        "locations = ['home', 'cell']\n",
        "\n",
        "# List to store output suffixes\n",
        "output_suffixes = []\n",
        "\n",
        "for weight in weights:\n",
        "    for is_log in logs:\n",
        "        for location in locations:\n",
        "            output_suffix = f\"{weight}_{'log' if is_log else 'nolog'}_{location}\"\n",
        "            output_suffixes.append(output_suffix)\n",
        "            process_cities(weight, is_log, location, output_suffix)"
      ],
      "metadata": {
        "id": "dkeprEgtPo1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result Analysis"
      ],
      "metadata": {
        "id": "UFfHoKePgX5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for output_suffix in output_suffixes:\n",
        "#     # Define the main directory and subdirectory paths\n",
        "#     main_directory = '/content/drive/MyDrive/network_radius_city_size'\n",
        "#     subdirectory = os.path.join(main_directory, output_suffix)\n",
        "\n",
        "#     # Create the subdirectory if it does not exist\n",
        "#     os.makedirs(subdirectory, exist_ok=True)\n",
        "\n",
        "#     # Construct the full file path\n",
        "#     file_path = os.path.join(subdirectory, f'network_radius_{output_suffix}.csv')\n",
        "\n",
        "#     data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "#     # Select numeric columns\n",
        "#     numeric_data = data.select_dtypes(include=[float, int])\n",
        "\n",
        "#     cols_to_select = list(range(0, 1)) + list(range(12, 21)) # not include left range\n",
        "#     filter_data = numeric_data.iloc[:, cols_to_select]\n",
        "\n",
        "#     # Calculate the correlation matrix\n",
        "#     correlation_matrix = filter_data.corr()\n",
        "\n",
        "#     plt.figure(figsize=(10, 8))\n",
        "#     sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "#     plt.title('Correlation Matrix')\n",
        "#     plt.savefig(os.path.join(subdirectory, f'correlation_matrix_{output_suffix}.png'))\n",
        "#     plt.show()\n",
        "\n",
        "#     # Scatterplot: City area and Social Network radius\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     sns.scatterplot(x='surfaceOfAdministrativeArea km2', y='Network Radius', data=data)\n",
        "\n",
        "#     # Label city names\n",
        "#     for i in range(data.shape[0]):\n",
        "#         plt.text(x=data['surfaceOfAdministrativeArea km2'][i],\n",
        "#                 y=data['Network Radius'][i],\n",
        "#                 s=data['City'][i],\n",
        "#                 fontdict=dict(color='red',size=10)\n",
        "#                 )\n",
        "\n",
        "#     plt.title('City Area vs. Network Radius')\n",
        "#     plt.xlabel('Surface of Administrative Area (km2)')\n",
        "#     plt.ylabel('Network Radius (km)')\n",
        "#     plt.savefig(os.path.join(subdirectory, f'city_area_vs_network_radius_{output_suffix}.png'))\n",
        "#     plt.show()\n",
        "\n",
        "#     # Scatterplot: Population and Social Network radius\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     sns.scatterplot(x='Population 2010', y='Network Radius', data=data)\n",
        "\n",
        "#     # Label city names\n",
        "#     for i in range(data.shape[0]):\n",
        "#         plt.text(x=data['Population 2010'][i],\n",
        "#                 y=data['Network Radius'][i],\n",
        "#                 s=data['City'][i],\n",
        "#                 fontdict=dict(color='red',size=10))\n",
        "\n",
        "#     plt.title('Population 2010 vs. Network Radius')\n",
        "#     plt.xlabel('Population 2010 (HAB)')\n",
        "#     plt.ylabel('Network Radius (km)')\n",
        "#     plt.savefig(os.path.join(subdirectory, f'population_2010_vs_network_radius_{output_suffix}.png'))\n",
        "#     plt.show()\n",
        "\n",
        "#     # Regression plot: city size vs. social network radius\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     sns.regplot(x='surfaceOfAdministrativeArea km2', y='Network Radius', data=data)\n",
        "#     plt.title('City Area vs. Network Radius with Regression Line')\n",
        "#     plt.xlabel('Surface of Administrative Area (km2)')\n",
        "#     plt.ylabel('Network Radius (km)')\n",
        "#     plt.savefig(os.path.join(subdirectory, f'city_area_vs_network_radius_regression_{output_suffix}.png'))\n",
        "#     plt.show()\n",
        "\n",
        "#     # Regression plot: population vs. social network radius\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     sns.regplot(x='Population 2010', y='Network Radius', data=data)\n",
        "#     plt.title('Population 2010 vs. Network Radius with Regression Line')\n",
        "#     plt.xlabel('Population 2010 (HAB)')\n",
        "#     plt.ylabel('Network Radius (km)')\n",
        "#     plt.savefig(os.path.join(subdirectory, f'population_2010_vs_network_radius_regression_{output_suffix}.png'))\n",
        "#     plt.show()\n"
      ],
      "metadata": {
        "id": "QIMShXjwgWCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the main directory and subdirectory paths\n",
        "main_directory = '/content/drive/MyDrive/network_radius_city_size'\n",
        "subdirectory = os.path.join(main_directory, output_suffix)\n",
        "\n",
        "# Create the subdirectory if it does not exist\n",
        "os.makedirs(subdirectory, exist_ok=True)\n",
        "\n",
        "# Construct the full file path\n",
        "file_path = os.path.join(subdirectory, f'network_radius_{output_suffix}.csv')\n",
        "\n",
        "print(file_path)\n",
        "\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "3kd5lDcWik0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data = data.select_dtypes(include=[float, int])\n",
        "\n",
        "\n",
        "cols_to_select = list(range(0, 1)) + list(range(17, 26)) # not include left range\n",
        "filter_data = numeric_data.iloc[:, cols_to_select]\n",
        "\n",
        "correlation_matrix = filter_data.corr()\n",
        "\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "DcxXUQsbhFMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.savefig(os.path.join(subdirectory, f'correlation_matrix_{output_suffix}.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Vr6XH2ShHtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression plot: city size vs. social network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='surfaceOfAdministrativeArea km2', y='Network Radius', data=data)\n",
        "plt.title('City Area vs. Network Radius with Regression Line')\n",
        "plt.xlabel('Surface of Administrative Area (km2)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'area_regression_{output_suffix}.png'))\n",
        "plt.show()\n",
        "\n",
        "# Regression plot: population vs. social network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='Population 2010', y='Network Radius', data=data)\n",
        "plt.title('Population 2010 vs. Network Radius with Regression Line')\n",
        "plt.xlabel('Population 2010 (HAB)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'population_regression_{output_suffix}.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "StQq7DRvhrCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot: City area and Social Network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='surfaceOfAdministrativeArea km2', y='Network Radius', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['surfaceOfAdministrativeArea km2'][i],\n",
        "             y=data['Network Radius'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10)\n",
        "             )\n",
        "\n",
        "plt.title('City Area vs. Network Radius')\n",
        "plt.xlabel('Surface of Administrative Area (km2)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'area_scatter_{output_suffix}.png'))\n",
        "plt.show()\n",
        "\n",
        "# Scatterplot: Population and Social Network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Population 2010', y='Network Radius', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['Population 2010'][i],\n",
        "             y=data['Network Radius'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10))\n",
        "\n",
        "plt.title('Population 2010 vs. Network Radius')\n",
        "plt.xlabel('Population 2010 (HAB)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'population_scatter_{output_suffix}.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qlnHr-v0jm_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Selection of independent and dependent variables\n",
        "X = numeric_data[['Population 2010', 'surfaceOfAdministrativeArea km2']]\n",
        "y = numeric_data['Network Radius']\n",
        "\n",
        "# Adding a constant term\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# fit a regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Output regression results\n",
        "print(model.summary())\n",
        "\n",
        "with open(os.path.join(subdirectory, f'regression_summary_{output_suffix}.txt'), \"w\") as f:\n",
        "    f.write(model.summary().as_text())\n",
        "    f.write(\"\\n\")\n",
        "    f.write(model.summary().as_latex())"
      ],
      "metadata": {
        "id": "wjhJvcqFiqHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUjSxh4JmgYt"
      },
      "source": [
        "# Plot the heatmap and antenna locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpRVvo8WmgYt"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "\n",
        "locations = []\n",
        "for user, cellid in home_locations.items():\n",
        "    if cellid in antenna_locations:\n",
        "        lat, lon = antenna_locations[cellid]\n",
        "        locations.append([lat, lon])\n",
        "\n",
        "map_center = [locations[0][0], locations[0][1]]\n",
        "map = folium.Map(location=map_center, zoom_start=12)\n",
        "\n",
        "HeatMap(locations).add_to(map)\n",
        "\n",
        "map.save('home_location_heatmap.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWfl1BuxmgYt"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "\n",
        "map_center = list(antenna_locations.values())[0]\n",
        "\n",
        "m = folium.Map(location=map_center, zoom_start=10)\n",
        "\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "for cellid, (lat, lon) in antenna_locations.items():\n",
        "    folium.Marker(\n",
        "        location=[lat, lon],\n",
        "        popup=str(cellid),\n",
        "        icon=folium.Icon(color='blue', icon='info-sign')\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "m.save('antennas_locations_map_with_clusters.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI7b_HjxmgYt"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "\n",
        "# Collect latitudes and longitudes for all home locations for the heatmap\n",
        "locations = []\n",
        "\n",
        "# Calculate the number of occurrences of each cellid to be used as the number of users\n",
        "cellid_counts = {}\n",
        "\n",
        "for user, cellid in home_locations.items():\n",
        "    if cellid in antenna_locations:\n",
        "        lat, lon = antenna_locations[cellid]\n",
        "        locations.append([lat, lon])\n",
        "        cellid_counts[cellid] = cellid_counts.get(cellid, 0) + 1\n",
        "\n",
        "# If there are home locations, choose the first one as the map center; otherwise, choose the first antenna locations\n",
        "map_center = locations[0] if locations else list(antenna_locations.values())[0]\n",
        "\n",
        "# Create a folium map instance\n",
        "m = folium.Map(location=map_center, zoom_start=12)\n",
        "\n",
        "# Add a heatmap layer for home locations\n",
        "HeatMap(locations).add_to(m)\n",
        "\n",
        "# Create a MarkerCluster object\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add markers for all antenna locations\n",
        "for cellid, (lat, lon) in antenna_locations.items():\n",
        "    # Get the users number using cellid_counts\n",
        "    users_count = cellid_counts.get(cellid, 0)\n",
        "\n",
        "    popup_text = f\"Cell ID: {cellid}<br>Users: {users_count}\"\n",
        "    folium.Marker(\n",
        "        location=[lat, lon],\n",
        "        popup=popup_text,  # Popup shows the popup_text\n",
        "        icon=folium.Icon(color='blue', icon='info-sign')  # Customize marker style\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "m.save('map_with_heatmap_and_clusters.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate home location, social size and plot with filter"
      ],
      "metadata": {
        "id": "m9qiqw3BC3ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract call records without filter and with filter"
      ],
      "metadata": {
        "id": "y1_b6vpnPF2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caculate the social range for each user"
      ],
      "metadata": {
        "id": "zmHTXEBSP3ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Do we need to filter the call made from outside the home location?\n",
        "\n",
        "# Social range\n",
        "social_ranges = {}\n",
        "\n",
        "# Dictionary to hold the distance between\n",
        "network_edges = {}\n",
        "\n",
        "for line in open(file_path_cdr_caucaia, 'r'):\n",
        "    parts = line.strip().split(';')\n",
        "    caller_id = parts[4]\n",
        "    called_id = parts[6]\n",
        "\n",
        "    if caller_id in home_locations and called_id in home_locations:\n",
        "        caller_home = home_locations[caller_id]\n",
        "        called_home = home_locations[called_id]\n",
        "        if caller_home in antenna_locations and called_home in antenna_locations:\n",
        "            lat1, lon1 = antenna_locations[caller_home]\n",
        "            lat2, lon2 = antenna_locations[called_home]\n",
        "            distance = haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "            if caller_id not in network_edges:\n",
        "                network_edges[caller_id] = {}\n",
        "            if called_id not in network_edges[caller_id]:\n",
        "                network_edges[caller_id][called_id] = distance\n",
        "            else:\n",
        "                network_edges[caller_id][called_id] += distance\n",
        "\n",
        "# Sum distances and calculate average range\n",
        "for caller, calls in network_edges.items():\n",
        "    total_distance = sum(calls.values())\n",
        "    total_calls = sum(user_calls[caller].values())\n",
        "    social_ranges[caller] = total_distance / total_calls if total_calls else 0\n",
        "\n",
        "# Output results\n",
        "for user, range_km in social_ranges.items():\n",
        "    print(f'User {user} has a social range of {range_km:.2f} km')"
      ],
      "metadata": {
        "id": "yRbzd5crKGYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0pGZj6AmgYh"
      },
      "source": [
        "## 1.1 Extract each user's call records {(User ID -> {CELLID -> Number of Calls}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUuhSpUymgYi"
      },
      "source": [
        "#### 1.3 Plot the distribution and boxplot of the total number of phone call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqUe4ZjdmgYi"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution\n",
        "total_calls_users = [sum(calls.values()) for calls in user_calls.values()]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(total_calls_users, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Total Call Counts per User')\n",
        "plt.xlabel('Total Call Counts')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "# plt.xscale('log')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb90tk0amgYi"
      },
      "outputs": [],
      "source": [
        "plt.boxplot(total_calls_users, vert=True, patch_artist=True,\n",
        "            flierprops=dict(marker='o', markerfacecolor='red', markersize=3),\n",
        "            showmeans=True)\n",
        "\n",
        "plt.title('Boxplot of Total Call Counts per User')\n",
        "plt.ylabel('Total Call Counts')\n",
        "# plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqP5OIJkmgYj"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(total_calls_users).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQfBRVsmmgYj"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of unique callees per caller\n",
        "unique_calls_per_user = {user: len(calls) for user, calls in social_network.items()}\n",
        "\n",
        "# Count how many users called exactly 1, 2, 3, 4, 5 unique users\n",
        "call_count_distribution = collections.Counter(unique_calls_per_user.values())\n",
        "\n",
        "print(f\"We have totally {len(unique_calls_per_user)} users who made call.\")\n",
        "\n",
        "total_1_to_5 = sum(call_count_distribution[i] for i in range(1, 6))\n",
        "print(f\"Number of users calling between 1 and 5 unique people: {total_1_to_5}\")\n",
        "\n",
        "percentage_1_to_5 = (total_1_to_5 / total_users) * 100\n",
        "print(f\"Percentage of users calling between 1 and 5 unique people: {percentage_1_to_5:.2f}%\")\n",
        "\n",
        "for i in range(1, 6):\n",
        "    print(f\"Number of unique users call {i} unique people: {call_count_distribution[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframe analysis for smaller cities"
      ],
      "metadata": {
        "id": "cjTWX3VXgogU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read CDR data to daraframe\n",
        "file_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/cdr_Caucaia.txt'\n",
        "\n",
        "# Define the indices and names of the columns to be used\n",
        "columns_to_use = [0, 1, 2, 4, 6, 7]  # Adjust according to your actual data structure\n",
        "column_names = ['date', 'time', 'duration', 'caller_id', 'callee_id', 'cell_id']\n",
        "\n",
        "# Specify data types for each column to optimize memory usage\n",
        "dtype = {\n",
        "    'duration': 'float32',  # Assuming duration is a floating number\n",
        "    'caller_id': 'category',  # Use 'category' data type to reduce memory usage for repetitive strings\n",
        "    'callee_id': 'category',\n",
        "    'cell_id': 'category'\n",
        "}\n",
        "\n",
        "# Read the data including only the required columns and specifying the date-time format\n",
        "df_cdr = pd.read_csv(\n",
        "    file_path_cdr,\n",
        "    sep=';',\n",
        "    header=None,\n",
        "    usecols=columns_to_use,\n",
        "    dtype=dtype,\n",
        "    names=column_names,\n",
        "    parse_dates=[['date', 'time']],\n",
        "    date_format='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "print(df_cdr.head())\n",
        "\n",
        "# read antenna locations\n",
        "# file_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/antennas_Franca.txt'\n",
        "# df_ana = read_ana(file_path_ana)\n",
        "\n"
      ],
      "metadata": {
        "id": "3F-XGi60gugV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cdr.info()"
      ],
      "metadata": {
        "id": "y-Svwq_cjZV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = df_cdr['date_time'].min()\n",
        "end_date = df_cdr['date_time'].max()\n",
        "total_days = (end_date - start_date).days + 1\n",
        "\n",
        "print(f\"Date range: {start_date} to {end_date}, {total_days} days in total.\")"
      ],
      "metadata": {
        "id": "H7g74IPFjv3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data by day and sum durations\n",
        "daily_duration = df_cdr.groupby(df_cdr['date_time'].dt.date)['duration'].sum()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "daily_duration.plot(kind='bar', color='blue', alpha=0.7)\n",
        "\n",
        "plt.title('Total Call Duration Over 30 Days')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Duration in Minutes')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate date labels for better visibility\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QQNWzItcojEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a 'day_of_week' column to the DataFrame\n",
        "df_cdr['day_of_week'] = df_cdr['date_time'].dt.dayofweek\n",
        "\n",
        "# Group data by 'day_of_week' and sum durations\n",
        "weekly_duration = df_cdr.groupby('day_of_week')['duration'].sum()\n",
        "\n",
        "# Map the 'day_of_week' integers to day names for clearer x-axis labels\n",
        "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
        "weekly_duration.index = weekly_duration.index.map(day_names)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "weekly_duration.plot(kind='bar', color='blue', alpha=0.7)\n",
        "\n",
        "plt.title('Total Call Duration by Day of the Week')\n",
        "plt.xlabel('Days of the Week')\n",
        "plt.ylabel('Total Duration in Minutes')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate day labels for better visibility\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WGNdhwKKpCEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data by day and count the number of calls\n",
        "daily_calls = df_cdr.groupby(df_cdr['date_time'].dt.date).size()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "daily_calls.plot(kind='bar', color='orange', alpha=0.7)\n",
        "\n",
        "plt.title('Total Call Counts Over 30 Days')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Calls')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate date labels for better visibility\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nPegNwWLpvnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group data by 'day_of_week' and count calls\n",
        "weekly_calls = df_cdr.groupby('day_of_week').size()\n",
        "\n",
        "# Map the 'day_of_week' integers to day names for clearer x-axis labels\n",
        "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
        "weekly_calls.index = weekly_calls.index.map(day_names)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "weekly_calls.plot(kind='bar', color='orange', alpha=0.7)\n",
        "\n",
        "plt.title('Total Call Counts by Day of the Week')\n",
        "plt.xlabel('Days of the Week')\n",
        "plt.ylabel('Number of Calls')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate day labels for better visibility\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4qB6wnHhqSMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define bins for the duration intervals\n",
        "bins = [0, 1, 5, 10, 30, 60, 120, 240, 480, float('inf')]  # Adjust the bins as needed\n",
        "labels = ['0-1 min', '1-5 mins', '5-10 mins', '10-30 mins', '30-60 mins', '1-2 hrs', '2-4 hrs', '4-8 hrs', 'Over 8 hrs']\n",
        "\n",
        "# Categorize durations into bins\n",
        "df_cdr['duration_category'] = pd.cut(df_cdr['duration'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Count the number of calls in each duration category\n",
        "duration_distribution = df_cdr['duration_category'].value_counts().sort_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = duration_distribution.plot(kind='bar', color='blue')\n",
        "plt.title('Distribution of Call Durations')\n",
        "plt.xlabel('Duration Intervals')\n",
        "plt.ylabel('Number of Calls')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding text labels above the bars\n",
        "for bar in bars.patches:\n",
        "    # Using bar.get_height() to get the height of the bar,\n",
        "    # and bar.get_x() + bar.get_width() / 2 to calculate center position of the bar\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{int(bar.get_height())}',\n",
        "             ha='center', va='bottom', color='black', fontsize=9)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mTW3q0sLqmla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter calls that last less than 60 seconds\n",
        "short_calls = df_cdr[df_cdr['duration'] < 1]\n",
        "\n",
        "# Plotting the distribution of call durations under one minute\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(short_calls['duration'], bins=30, color='blue', kde=True)  # Bins for every 2 seconds\n",
        "plt.title('Distribution of Call Durations Under One Minute')\n",
        "plt.xlabel('Duration in Seconds')\n",
        "plt.ylabel('Number of Calls')\n",
        "plt.xticks([i/60 for i in range(0, 61, 5)], [f\"{i}s\" for i in range(0, 61, 5)])  # Setting x-ticks to show seconds\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N6K1E5ILuTTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define bins for the duration intervals\n",
        "bins = [0, 1, 5, 10, 30, 60, 120, 240, 480, float('inf')]  # Adjust the bins as needed\n",
        "labels = ['0-1 min', '1-5 mins', '5-10 mins', '10-30 mins', '30-60 mins', '1-2 hrs', '2-4 hrs', '4-8 hrs', 'Over 8 hrs']\n",
        "\n",
        "# Categorize durations into bins\n",
        "df_cdr['duration_category'] = pd.cut(df_cdr['duration'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Create a box plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='duration_category', y='duration', data=df_cdr, palette='Blues')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Box Plot of Call Durations')\n",
        "plt.xlabel('Duration Intervals')\n",
        "plt.ylabel('Call Duration (minutes)')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AYFlDCHTBVgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum durations per caller_id\n",
        "total_durations = df_cdr.groupby('caller_id')['duration'].sum()\n",
        "\n",
        "# Plotting the total durations. Since there are potentially millions of callers,\n",
        "# we use a histogram and a KDE plot for a better understanding of distribution.\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(total_durations, bins=100, kde=True, color='green')\n",
        "plt.title('Distribution of Total Call Duration Per User')\n",
        "plt.xlabel('Total Duration (minutes)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xscale('log')  # Use logarithmic scale if the range is wide\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nYmGut_ltKuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df_cdr is your DataFrame and it includes a 'caller_id' column\n",
        "call_counts = df_cdr['caller_id'].value_counts()\n",
        "\n",
        "# Convert the Series to a DataFrame for easier handling\n",
        "call_counts_df = call_counts.reset_index()\n",
        "call_counts_df.columns = ['caller_id', 'number_of_calls']"
      ],
      "metadata": {
        "id": "IbrMug5hfzqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distribution of call counts\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(call_counts, bins=100, color='blue', range=(1, call_counts.quantile(0.99)))  # Adjust range and bins as needed\n",
        "plt.title('Distribution of Call Counts per User')\n",
        "plt.xlabel('Number of Calls')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xscale('log')  # Use logarithmic scale if the data spans several orders of magnitude\n",
        "# plt.yscale('log')  # Log scale for y-axis can also be helpful in skewed distributions\n",
        "plt.show()\n",
        "\n",
        "# Show basic statistics\n",
        "print(call_counts_df['number_of_calls'].describe())\n"
      ],
      "metadata": {
        "id": "T_IOTRZ3f53Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a small constant if there are any zero call counts (unlikely in this case)\n",
        "call_counts_log = np.log1p(call_counts)\n",
        "\n",
        "# Plotting the log-transformed data\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(call_counts_log, bins=50, kde=False, color='green')\n",
        "plt.title('Log-Transformed Distribution of Call Counts per User')\n",
        "plt.xlabel('Log of Number of Calls')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Os3OJjHVgSrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))  # Setup the plot size\n",
        "stats.probplot(call_counts_log, dist=\"norm\", plot=ax)  # Generate and plot the Q-Q plot\n",
        "\n",
        "# Customize the plot\n",
        "ax.get_lines()[0].set_color('blue')  # Change the color of the points to blue\n",
        "ax.get_lines()[1].set_color('red')   # Change the color of the line to red\n",
        "ax.set_title('Q-Q plot of Log-Transformed Call Durations')\n",
        "ax.set_xlabel('Theoretical Quantiles')\n",
        "ax.set_ylabel('Ordered Values')\n",
        "\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "_7x5JMZXhQqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the threshold for call counting\n",
        "lower_threshold = 5\n",
        "upper_threshold = 200\n",
        "\n",
        "# Apply thresholds\n",
        "filtered_call_counts = df_cdr['caller_id'].value_counts()\n",
        "total_calls_original = df_cdr['caller_id'].value_counts().sum()\n",
        "filtered_call_counts = filtered_call_counts[(filtered_call_counts >= lower_threshold) & (filtered_call_counts <= upper_threshold)]\n",
        "filtered_log_counts = np.log1p(filtered_call_counts)\n",
        "\n",
        "total_calls_filtered = filtered_call_counts.sum()\n",
        "\n",
        "percentage_filtered = (total_calls_filtered / total_calls_original) * 100\n",
        "\n",
        "print(f\"Percentage of calls remaining after filtering: {percentage_filtered:.2f}%\")\n",
        "\n",
        "# Re-plotting histograms and Q-Q plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(filtered_log_counts, kde=True)\n",
        "plt.title('Filtered Histogram of Log-Transformed Call Counts')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "stats.probplot(filtered_log_counts, dist=\"norm\", plot=plt)\n",
        "plt.show()\n",
        "\n",
        "# D'Agostino's K^2 test\n",
        "k2, p = stats.normaltest(filtered_log_counts)\n",
        "print(f\"Statistic: {k2}, p-value: {p}\")\n",
        "\n",
        "# Shapiro-Wilk test\n",
        "stat, p = stats.shapiro(filtered_log_counts.sample(5000))\n",
        "print(f\"Shapiro-Wilk Test: Statistic={stat}, p-value={p}\")"
      ],
      "metadata": {
        "id": "odjWDcnrlTXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "call_counts = df_cdr['caller_id'].value_counts()\n",
        "\n",
        "# Define different thresholds for testing\n",
        "thresholds = range(1, 15)\n",
        "results = []\n",
        "\n",
        "for lower_threshold in thresholds:\n",
        "    # Applying thresholds to filter data\n",
        "    filtered_call_counts = call_counts[call_counts >= lower_threshold]\n",
        "    filtered_log_counts = np.log1p(filtered_call_counts)\n",
        "\n",
        "    # Calculating statistical tests\n",
        "    k2, p_normaltest = stats.normaltest(filtered_log_counts)\n",
        "    stat_shapiro, p_shapiro = stats.shapiro(filtered_log_counts.sample(min(5000, len(filtered_log_counts))))  # 样本量过大时需要抽样\n",
        "\n",
        "    # Storing results\n",
        "    results.append({\n",
        "        'lower_threshold': lower_threshold,\n",
        "        'p_value_normaltest': p_normaltest,\n",
        "        'p_value_shapiro': p_shapiro\n",
        "    })\n",
        "\n",
        "# Converting results to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Plotting p-values against thresholds\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(results_df['lower_threshold'], results_df['p_value_normaltest'], label='D\\'Agostino\\'s K^2 Test p-value', marker='o')\n",
        "plt.plot(results_df['lower_threshold'], results_df['p_value_shapiro'], label='Shapiro-Wilk Test p-value', marker='x')\n",
        "plt.xlabel('Lower Threshold')\n",
        "plt.ylabel('p-value')\n",
        "plt.title('Effect of Lower Threshold on Normality Tests')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eae-JUFNmqbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "60lWnbzUklSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6wyUPb2dGXYj",
        "cdtxUH1iyFI7",
        "1uq-rsbPr5Py",
        "gUjSxh4JmgYt",
        "m9qiqw3BC3ps",
        "zmHTXEBSP3ye",
        "cjTWX3VXgogU"
      ],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}