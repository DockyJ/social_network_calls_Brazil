{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploring the relationship between urban size and social network size using call detail records in Brazil\n",
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0V7RHHymgYg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os, csv, collections, datetime\n",
        "# import networkx as nx\n",
        "from math import radians, cos, sin, asin, sqrt, log1p\n",
        "import unicodedata, gc\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import normaltest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc7msXGlnETT"
      },
      "outputs": [],
      "source": [
        "# Load Google shared folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gtt5hXeO_sj"
      },
      "source": [
        "# Common Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc0EKpl8GZze"
      },
      "outputs": [],
      "source": [
        "def normalize_string(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn').lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMJNKznImgYj"
      },
      "outputs": [],
      "source": [
        "def read_ana(file_path_ana):\n",
        "    \"\"\"\n",
        "    Read the antenna locations file and return a dictionary of antenna locations.\n",
        "    \"\"\"\n",
        "    # Load antenna locations\n",
        "    antenna_locations = {}\n",
        "\n",
        "    with open(file_path_ana, 'r') as file:\n",
        "        reader = csv.reader(file, delimiter=';')\n",
        "        next(reader)  # Skip the header\n",
        "        for row in reader:\n",
        "            cell_id, lat, lon = row[0], float(row[3]), float(row[4])\n",
        "            antenna_locations[cell_id] = (lat, lon)\n",
        "    return antenna_locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B59tL11mgYk"
      },
      "outputs": [],
      "source": [
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate the great circle distance between two points\n",
        "    on the earth (specified in decimal degrees)\n",
        "    \"\"\"\n",
        "    # Convert decimal degrees to radians\n",
        "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
        "\n",
        "    # Haversine formula\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
        "    c = 2 * asin(sqrt(a))\n",
        "    return c * 6371 # Radius of earth in kilometers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIPfe1l26Iy4"
      },
      "outputs": [],
      "source": [
        "def home_location_calculation(calls_users_time_filter, significant_users):\n",
        "    \"\"\"\n",
        "    Calculate the home location based on the filtered records.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    home_locations = {}\n",
        "\n",
        "    for user, locations in calls_users_time_filter.items():\n",
        "        if user in significant_users:\n",
        "            most_frequent_cell, max_calls = max(locations.items(), key=lambda item: item[1])\n",
        "            total_calls = sum(locations.values())\n",
        "            if max_calls >= total_calls * 0.5:\n",
        "                home_locations[user] = (most_frequent_cell, max_calls)\n",
        "    return home_locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmlE3haxnjj1"
      },
      "outputs": [],
      "source": [
        "def write_data_to_csv(df, city, city_data, output_suffix):\n",
        "    \"\"\"\n",
        "    Write the data to a CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the main directory and subdirectory paths\n",
        "    main_directory = '/content/drive/MyDrive/network_radius_city_size'\n",
        "    subdirectory = os.path.join(main_directory, output_suffix)\n",
        "\n",
        "    # Create the subdirectory if it does not exist\n",
        "    os.makedirs(subdirectory, exist_ok=True)\n",
        "\n",
        "    # Construct the full file path\n",
        "    file_path = os.path.join(subdirectory, f'network_radius_{output_suffix}.csv')\n",
        "\n",
        "    # Determine the file mode based on header_written\n",
        "    file_mode = 'w' if not header_written else 'a'\n",
        "\n",
        "    selected_row = df[df['City_lower'] == normalize_string(city)].iloc[0]\n",
        "    data_to_write = [\n",
        "        city,\n",
        "        *city_data,\n",
        "        selected_row['Population 2010'],\n",
        "        selected_row['DENS HAB KM2'],\n",
        "        selected_row['Income 2010'],\n",
        "        selected_row['GDP 2010'],\n",
        "        selected_row['GDP 2017'],\n",
        "        selected_row['IDHM 2010'],\n",
        "        selected_row['Comp Total Ruas'],\n",
        "        selected_row['Number Of Deaths By Traffic Accident'],\n",
        "        selected_row['surfaceOfAdministrativeArea km2']\n",
        "    ]\n",
        "\n",
        "    with open(file_path, file_mode, newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if not header_written:\n",
        "                writer.writerow([\n",
        "                    'City', 'Network Radius',\n",
        "                    'Total Users',\n",
        "                    'Users after Filter 1', 'Percentage Filtered 1 Users',\n",
        "                    'Significant Users', 'Percentage Significant Users',\n",
        "                    'Final Users', 'Percentage Final Users',\n",
        "                    'Total Calls Weight', 'Final Calls Weight',\n",
        "                    'Percentage Final Calls Weight',\n",
        "                    'Records Callee Missed Home Location',\n",
        "                    'Zero Radius Users', 'Percentage Zero Radius Users',\n",
        "                    'Spam Calls', 'Percentage Spam Calls',\n",
        "                    'Number of Antennas',\n",
        "\n",
        "                    'Population 2010', 'DENS HAB KM2',\n",
        "                    'Income 2010', 'GDP 2010',\n",
        "                    'GDP 2017', 'IDHM 2010',\n",
        "                    'Comp Total Ruas', 'Number Of Deaths By Traffic Accident',\n",
        "                    'surfaceOfAdministrativeArea km2'\n",
        "                ])\n",
        "        writer.writerow(data_to_write)\n",
        "    print(f\"Finish writing radius of {city}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHei6i597ocX"
      },
      "outputs": [],
      "source": [
        "def network_radius_calculation_individual(calls_between_users, home_locations, antenna_locations, city, output_suffix, is_log=False, non_zero=False):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on individual callers' network radii and then average.\n",
        "\n",
        "    Args:\n",
        "        calls_between_users: Dictionary with callers and their callees.\n",
        "        home_locations: Dictionary mapping users to their home location (cell).\n",
        "        antenna_locations: Dictionary mapping cell IDs to (latitude, longitude).\n",
        "        city: The name of the city for saving plots.\n",
        "        is_log: Whether to apply a log transform to the call weights.\n",
        "\n",
        "    Returns:\n",
        "        Average network radius for the city, total weight, number of final users, and number of users with missing home locations.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    individual_radii = []\n",
        "    total_weight_final = 0.0\n",
        "    callee_miss_home_record = 0\n",
        "    zero_radius_count = 0\n",
        "\n",
        "    # FOR Distribution analysis\n",
        "    # individual_radii_nozero=[]\n",
        "\n",
        "    # Calculate network radius for each caller\n",
        "    for caller, callees in calls_between_users.items():\n",
        "        if caller in home_locations:\n",
        "            caller_home = home_locations[caller][0]\n",
        "            if caller_home in antenna_locations:\n",
        "                lat1, lon1 = antenna_locations[caller_home]\n",
        "                distances = []\n",
        "                total_weight = 0\n",
        "                distances_nozero = []\n",
        "\n",
        "                for callee, call_weight in callees.items():\n",
        "                    if callee in home_locations:\n",
        "                        callee_home = home_locations[callee][0]\n",
        "                        if callee_home in antenna_locations:\n",
        "                            lat2, lon2 = antenna_locations[callee_home]\n",
        "                            distance = haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "                            if distance == 0:\n",
        "                                distance = 0.5\n",
        "\n",
        "                                # FOR Distribution analysis\n",
        "                            #     distance_nozero = 0.5\n",
        "                            # else:\n",
        "                            #     distance_nozero = distance\n",
        "\n",
        "                            # Calculate weighted distance\n",
        "                            distances.append(distance * call_weight)\n",
        "\n",
        "                            # FOR Distribution analysis\n",
        "                            # distances_nozero.append(distance_nozero * call_weight)\n",
        "\n",
        "                            total_weight += call_weight\n",
        "                    else:\n",
        "                        # callee_miss_home contains duplicated callees as it is actually call records of missing callees\n",
        "                        callee_miss_home_record += 1\n",
        "\n",
        "                # Calculate the individual's network radius\n",
        "                if total_weight > 0:\n",
        "                    individual_radius = sum(distances) / len(callees)\n",
        "\n",
        "                    # FOR Distribution analysis\n",
        "                    # individual_radius_nozero = sum(distances_nozero) / len(callees)\n",
        "                    # individual_radii_nozero.append(individual_radius_nozero)\n",
        "\n",
        "                    if is_log:\n",
        "                        individual_radius = np.log1p(individual_radius)\n",
        "\n",
        "                    if individual_radius == 0:\n",
        "                        zero_radius_count += 1\n",
        "                        if not non_zero:  # Only append if non_zero is False\n",
        "                            individual_radii.append(individual_radius)\n",
        "                    else:\n",
        "                        individual_radii.append(individual_radius)\n",
        "                    total_weight_final += total_weight\n",
        "\n",
        "    # Calculate the average network radius\n",
        "    network_radius = np.mean(individual_radii) if individual_radii else 0\n",
        "    total_users_final = len(individual_radii)\n",
        "\n",
        "    # Plot the distribution of individual network radii\n",
        "    # plot_network_radius_distribution(individual_radii, individual_radii_nozero, city, output_suffix)\n",
        "\n",
        "    return network_radius, total_weight_final, total_users_final, callee_miss_home_record, zero_radius_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NyER5yK7jXA"
      },
      "outputs": [],
      "source": [
        "def plot_network_radius_distribution(individual_radii, individual_radii_nozero, city, output_suffix):\n",
        "    \"\"\"\n",
        "    Plot the distribution of individual network radii and save the plot.\n",
        "\n",
        "    Args:\n",
        "        individual_radii: List of individual network radii.\n",
        "        individual_radii_nozero: List of individual network radii excluding zero values.\n",
        "        city: The name of the city for saving plots.\n",
        "        output_suffix: Suffix for the output file names.\n",
        "    \"\"\"\n",
        "    main_directory = '/content/drive/MyDrive/network_radius_city_size'\n",
        "    subdirectory_1 = os.path.join(main_directory, city)\n",
        "    os.makedirs(subdirectory_1, exist_ok=True)\n",
        "\n",
        "    subdirectory = os.path.join(subdirectory_1, output_suffix)\n",
        "    # subdirectory = os.path.join(subdirectory, '_0.1')\n",
        "    os.makedirs(subdirectory, exist_ok=True)\n",
        "\n",
        "    # Construct the file paths\n",
        "    hist_file_path = os.path.join(subdirectory, f'network_radius_distribution_{city}_{output_suffix}.png')\n",
        "    log_hist_file_path = os.path.join(subdirectory, f'log_network_radius_distribution_{city}_{output_suffix}.png')\n",
        "    qq_file_path = os.path.join(subdirectory, f'qq_plot_{city}_{output_suffix}.png')\n",
        "\n",
        "    hist_file_path_nozero = os.path.join(subdirectory, f'network_radius_distribution_nozero_{city}_{output_suffix}.png')\n",
        "    log_hist_file_path_nozero = os.path.join(subdirectory, f'log_network_radius_distribution_nozero_{city}_{output_suffix}.png')\n",
        "    qq_file_path_nozero = os.path.join(subdirectory, f'qq_plot_nozero_{city}_{output_suffix}.png')\n",
        "\n",
        "    sqrt_hist_file_path = os.path.join(subdirectory, f'sqrt_network_radius_distribution_{city}_{output_suffix}.png')\n",
        "    sqrt_hist_file_path_nozero = os.path.join(subdirectory, f'sqrt_network_radius_distribution_nozero_{city}_{output_suffix}.png')\n",
        "    sqrt_qq_file_path = os.path.join(subdirectory, f'sqrt_qq_plot_{city}_{output_suffix}.png')\n",
        "    sqrt_qq_file_path_nozero = os.path.join(subdirectory, f'sqrt_qq_plot_nozero_{city}_{output_suffix}.png')\n",
        "\n",
        "    # Plot the distribution for non-zero values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(individual_radii_nozero, bins=30, color='blue', alpha=0.7)\n",
        "    plt.title(f'Network Radius Distribution for {city} (Non-Zero Values)')\n",
        "    plt.xlabel('Network Radius (km)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary\n",
        "    desc_stats = pd.Series(individual_radii_nozero).describe()\n",
        "    stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Statistical Summary:\\n{stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(hist_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the log-transformed distribution for non-zero values\n",
        "    log_radii_nozero = np.log1p(individual_radii_nozero)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(log_radii_nozero, bins=30, color='green', alpha=0.7)\n",
        "    plt.title(f'Log-Transformed Network Radius Distribution for {city} (Non-Zero Values)')\n",
        "    plt.xlabel('Log(Network Radius + 1)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary for log data\n",
        "    log_desc_stats = pd.Series(log_radii_nozero).describe()\n",
        "    log_stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in log_desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Log Statistical Summary:\\n{log_stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(log_hist_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Q-Q plot for non-zero values\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(log_radii_nozero, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Q-Q Plot of Log-Transformed Network Radius for {city} (Non-Zero Values)')\n",
        "\n",
        "    # Save the Q-Q plot\n",
        "    plt.savefig(qq_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the square root-transformed distribution for non-zero values\n",
        "    sqrt_radii_nozero = np.sqrt(individual_radii_nozero)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(sqrt_radii_nozero, bins=30, color='purple', alpha=0.7)\n",
        "    plt.title(f'Square Root-Transformed Network Radius Distribution for {city} (Non-Zero Values)')\n",
        "    plt.xlabel('Square Root(Network Radius)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary for sqrt data\n",
        "    sqrt_desc_stats = pd.Series(sqrt_radii_nozero).describe()\n",
        "    sqrt_stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in sqrt_desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Sqrt Statistical Summary:\\n{sqrt_stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(sqrt_hist_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Q-Q plot for sqrt-transformed distribution\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(sqrt_radii_nozero, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Q-Q Plot of Sqrt-Transformed Network Radius for {city} (Non-Zero Values)')\n",
        "\n",
        "    # Save the Q-Q plot\n",
        "    plt.savefig(sqrt_qq_file_path_nozero)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(individual_radii, bins=30, color='blue', alpha=0.7)\n",
        "    plt.title(f'Network Radius Distribution for {city}')\n",
        "    plt.xlabel('Network Radius (km)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary\n",
        "    desc_stats = pd.Series(individual_radii).describe()\n",
        "    stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Statistical Summary:\\n{stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(hist_file_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the log-transformed distribution\n",
        "    log_radii = np.log1p(individual_radii)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(log_radii, bins=30, color='green', alpha=0.7)\n",
        "    plt.title(f'Log-Transformed Network Radius Distribution for {city}')\n",
        "    plt.xlabel('Log(Network Radius + 1)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary for log data\n",
        "    log_desc_stats = pd.Series(log_radii).describe()\n",
        "    log_stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in log_desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Log Statistical Summary:\\n{log_stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(log_hist_file_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Q-Q plot\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(log_radii, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Q-Q Plot of Log-Transformed Network Radius for {city}')\n",
        "\n",
        "    # Save the Q-Q plot\n",
        "    plt.savefig(qq_file_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot the square root-transformed distribution\n",
        "    sqrt_radii = np.sqrt(individual_radii)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(sqrt_radii, bins=30, color='purple', alpha=0.7)\n",
        "    plt.title(f'Square Root-Transformed Network Radius Distribution for {city}')\n",
        "    plt.xlabel('Square Root(Network Radius)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Calculate and display statistical summary for sqrt data\n",
        "    sqrt_desc_stats = pd.Series(sqrt_radii).describe()\n",
        "    sqrt_stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in sqrt_desc_stats.items()])\n",
        "    plt.figtext(0.35, 0.5, f'Sqrt Statistical Summary:\\n{sqrt_stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(sqrt_hist_file_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Q-Q plot for sqrt-transformed distribution\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(sqrt_radii, dist=\"norm\", plot=plt)\n",
        "    plt.title(f'Q-Q Plot of Sqrt-Transformed Network Radius for {city}')\n",
        "\n",
        "    # Save the Q-Q plot\n",
        "    plt.savefig(sqrt_qq_file_path)\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5TQo-D6TMND"
      },
      "outputs": [],
      "source": [
        "def convert_dict_to_df_count(calls_dict):\n",
        "    # Prepare empty lists to fill with data\n",
        "    data = {'caller_id': [], 'id2': [], 'call_weight': []}\n",
        "\n",
        "    for caller_id, cells in calls_dict.items():\n",
        "        for id2, weight in cells.items():\n",
        "            data['caller_id'].append(caller_id)\n",
        "            data['id2'].append(id2)\n",
        "            data['call_weight'].append(weight)\n",
        "\n",
        "    # Create DataFrame from the collected data\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Convert data types to save memory\n",
        "    df['caller_id']  = df['caller_id'].astype('category')   # Reduces memory by using categorical data type for repetitive strings\n",
        "    df['id2']   = df['id2'].astype('category')        # Similar to caller_id, using category data type for cell ids\n",
        "    df['call_weight'] = df['call_weight'].astype('float32')   # Using float32 for call weights to accommodate decimal values\n",
        "\n",
        "    # Aggregate call weights by caller_id\n",
        "    df_summed = df.groupby('caller_id')['call_weight'].sum().reset_index()\n",
        "    df_summed.columns = ['caller_id', 'total_call_weight']\n",
        "\n",
        "    return df_summed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LIdWHIp1yNa"
      },
      "outputs": [],
      "source": [
        "def convert_dict_to_df_dura(calls_dict):\n",
        "    # Prepare empty lists to fill with data\n",
        "    data = {'caller_id': [], 'callee_id': [], 'total_duration': []}\n",
        "\n",
        "    # Aggregate durations by caller_id to callee_id\n",
        "    for caller_id, callees in calls_dict.items():\n",
        "        for callee_id, duration in callees.items():\n",
        "            data['caller_id'].append(caller_id)\n",
        "            data['callee_id'].append(callee_id)\n",
        "            data['total_duration'].append(duration)\n",
        "\n",
        "    # Create DataFrame from the collected data\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Convert data types to save memory and reflect correct types\n",
        "    df['caller_id'] = df['caller_id'].astype('category')  # Reduces memory for repetitive strings\n",
        "    df['callee_id'] = df['callee_id'].astype('category')  # Same as caller_id\n",
        "    df['total_duration'] = df['total_duration'].astype('float32')  # Ensures duration is in float format\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkVUHMNSJGeT"
      },
      "source": [
        "## Method 1: Utility Functions for using counts as the weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RgJvamS5Shk"
      },
      "outputs": [],
      "source": [
        "def read_cdr_count(file_path_cdr):\n",
        "    \"\"\"\n",
        "    Read the CDR file and return a dictionary of user call records,\n",
        "    significant user call records based on time conditions,\n",
        "    and calls between users.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to hold significant user call records based on time conditions to calculate the home location\n",
        "    calls_users_time_filter = {}\n",
        "    # Dictionary to store the calls between users' ids\n",
        "    calls_between_users = {}\n",
        "    # Spam call possibility\n",
        "    dura_spam = 0\n",
        "\n",
        "\n",
        "    # Read the data file line by line\n",
        "    with open(file_path_cdr, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into parts\n",
        "            parts = line.strip().split(';')\n",
        "\n",
        "            duration = float(parts[2])\n",
        "            if duration <= 0.1:\n",
        "                dura_spam += 1\n",
        "                continue\n",
        "\n",
        "            # Extract the columns of interest: Date, Time, Caller ID, Cell ID\n",
        "            date_time_str = parts[0] + ' ' + parts[1]\n",
        "            caller_id = parts[4]\n",
        "            callee_id = parts[6]\n",
        "            cell_id = parts[7]\n",
        "            # Parse date and time\n",
        "            date_time = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
        "            weekday = date_time.weekday()\n",
        "            hour = date_time.hour\n",
        "\n",
        "\n",
        "\n",
        "            # Store calls between users\n",
        "            if caller_id not in calls_between_users:\n",
        "                calls_between_users[caller_id] = {}\n",
        "            else:\n",
        "                if callee_id not in calls_between_users[caller_id]:\n",
        "                    calls_between_users[caller_id][callee_id] = 1\n",
        "                else:\n",
        "                    calls_between_users[caller_id][callee_id] += 1\n",
        "\n",
        "            # Filter based on time conditions\n",
        "            if (weekday < 5 and (hour >= 19 or hour <= 6)) or (weekday >= 5):\n",
        "                if caller_id not in calls_users_time_filter:\n",
        "                    calls_users_time_filter[caller_id] = {}\n",
        "\n",
        "                # Count calls per location for significant times\n",
        "                if cell_id not in calls_users_time_filter[caller_id]:\n",
        "                    calls_users_time_filter[caller_id][cell_id] = 1\n",
        "                else:\n",
        "                    calls_users_time_filter[caller_id][cell_id] += 1\n",
        "    return calls_users_time_filter, calls_between_users, dura_spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WydkvgsZ9EIA"
      },
      "outputs": [],
      "source": [
        "def significant_users_calculation(calls_users_time_filter):\n",
        "    \"\"\"\n",
        "    Calculate the significant users based on the filtered records.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    significant_users = set()\n",
        "\n",
        "    # Determine the significant users based on the filtered records\n",
        "    for user, locations in calls_users_time_filter.items():\n",
        "        # if user not in excluded_users and 5 <= sum(locations.values()) <= 200:\n",
        "        #     significant_users.add(user)\n",
        "        if 5 <= sum(locations.values()) <= 200:\n",
        "            significant_users.add(user)\n",
        "    return significant_users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yAHMrWGH4S5"
      },
      "source": [
        "## Method 2: Utility Functions for using duration as the weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji9_6GMH7Dis"
      },
      "outputs": [],
      "source": [
        "def read_cdr_duration(file_path_cdr):\n",
        "    \"\"\"\n",
        "    Read the CDR file and return a dictionary of user call records,\n",
        "    significant user call records based on time conditions,\n",
        "    and calls between users.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary for holding significant user call records based on time conditions to calculate the home location\n",
        "    calls_users_time_filter = {}\n",
        "    # Dictionary for accumulating duration from callers to the same callees to calculate the social network radius\n",
        "    calls_users_duration = {}\n",
        "    # Spam possibility\n",
        "    dura_spam = 0\n",
        "\n",
        "    # Read the data file line by line\n",
        "    with open(file_path_cdr, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split lines into parts\n",
        "            parts = line.strip().split(';')\n",
        "\n",
        "            # Filter out spam calls\n",
        "            duration = float(parts[2])\n",
        "            if duration <= 0.1:\n",
        "                dura_spam += 1\n",
        "                continue\n",
        "\n",
        "            # Extract columns of interest: Date, Time, Duration, call id, cell id\n",
        "            data_time_str = parts[0] + ' ' + parts[1]\n",
        "            caller_id = parts[4]\n",
        "            callee_id = parts[6]\n",
        "            cell_id = parts[7]\n",
        "\n",
        "            # Parse date and time\n",
        "            date_time = datetime.datetime.strptime(data_time_str, '%Y-%m-%d %H:%M:%S')\n",
        "            weekday = date_time.weekday()\n",
        "            hour = date_time.hour\n",
        "\n",
        "            # Filter based on time conditions for significant users determination\n",
        "            if (weekday < 5 and (hour >= 19 or hour <= 6)) or (weekday >= 5):\n",
        "                if caller_id not in calls_users_time_filter:\n",
        "                    calls_users_time_filter[caller_id] = {}\n",
        "\n",
        "                # Count calls per location for significant times\n",
        "                if cell_id not in calls_users_time_filter[caller_id]:\n",
        "                    calls_users_time_filter[caller_id][cell_id] = 1\n",
        "                else:\n",
        "                    calls_users_time_filter[caller_id][cell_id] += 1\n",
        "\n",
        "            # Accumulate duration for callers to callees\n",
        "            if caller_id not in calls_users_duration:\n",
        "                calls_users_duration[caller_id] = {}\n",
        "            else:\n",
        "                if callee_id not in calls_users_duration[caller_id]:\n",
        "                    calls_users_duration[caller_id][callee_id] = float(duration)\n",
        "                else:\n",
        "                    calls_users_duration[caller_id][callee_id] += float(duration)\n",
        "\n",
        "    return calls_users_time_filter, calls_users_duration, dura_spam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40xt9n05afb0"
      },
      "source": [
        "## Method 3: Utility Functions for using actual location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exwo6BcGaj2d"
      },
      "outputs": [],
      "source": [
        "def read_cdr_count_cell(file_path_cdr):\n",
        "    \"\"\"\n",
        "    Read the CDR file and return a dictionary of user call records,\n",
        "    significant user call records based on time conditions,\n",
        "    and calls between users.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to hold significant user call records based on time conditions to calculate the home location\n",
        "    calls_users_time_filter = {}\n",
        "    # List to store the call record\n",
        "    calls_between_users = []\n",
        "    # Spam call possibility\n",
        "    dura_spam = 0\n",
        "\n",
        "\n",
        "    # Read the data file line by line\n",
        "    with open(file_path_cdr, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into parts\n",
        "            parts = line.strip().split(';')\n",
        "\n",
        "            duration = float(parts[2])\n",
        "            if duration <= 0.1:\n",
        "                dura_spam += 1\n",
        "                continue\n",
        "\n",
        "            # Extract the columns of interest: Date, Time, Caller ID, Cell ID\n",
        "            date_time_str = parts[0] + ' ' + parts[1]\n",
        "            caller_id = parts[4]\n",
        "            callee_id = parts[6]\n",
        "            cell_id = parts[7]\n",
        "\n",
        "            # Parse date and time\n",
        "            date_time = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
        "            weekday = date_time.weekday()\n",
        "            hour = date_time.hour\n",
        "\n",
        "            # Store calls between users\n",
        "            calls_between_users.append({\n",
        "                'caller_id': caller_id,\n",
        "                'callee_id': callee_id,\n",
        "                'cell_id': cell_id,\n",
        "            })\n",
        "\n",
        "            # Filter based on time conditions\n",
        "            if (weekday < 5 and (hour >= 19 or hour <= 6)) or (weekday >= 5):\n",
        "                if caller_id not in calls_users_time_filter:\n",
        "                    calls_users_time_filter[caller_id] = {}\n",
        "\n",
        "                # Count calls per location for significant times\n",
        "                if cell_id not in calls_users_time_filter[caller_id]:\n",
        "                    calls_users_time_filter[caller_id][cell_id] = 1\n",
        "                else:\n",
        "                    calls_users_time_filter[caller_id][cell_id] += 1\n",
        "    return calls_users_time_filter, calls_between_users, dura_spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_KDV4RTs0wM"
      },
      "outputs": [],
      "source": [
        "def network_radius_calculation_cell(calls_between_users, home_locations, antenna_locations, city, output_suffix, is_log=False, non_zero=False):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on the calls between users using the caller's cell location\n",
        "    and the callee's home location, then average. Individually calculate the network radius for each caller.\n",
        "\n",
        "    Args:\n",
        "        calls_between_users: List of call records with caller_id, callee_id, and cell_id.\n",
        "        home_locations: Dictionary mapping users to their home location (cell).\n",
        "        antenna_locations: Dictionary mapping cell IDs to (latitude, longitude).\n",
        "        city: The name of the city for saving plots.\n",
        "        is_log: Whether to apply a log transform to the call weights.\n",
        "\n",
        "    Returns:\n",
        "        Average network radius for the city, total weight, number of final users, and number of users with missing home locations.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    individual_radii = []\n",
        "    total_weight_final = 0.0\n",
        "    callee_miss_home_record = 0\n",
        "    zero_radius_count = 0\n",
        "\n",
        "    # Create a dictionary to hold distances and weights per caller\n",
        "    caller_distances = {}\n",
        "\n",
        "    # FOR Distribution analysis\n",
        "    # individual_radii_nozero = []\n",
        "\n",
        "    # Calculate network radius for each caller\n",
        "    for call_record in calls_between_users:\n",
        "        caller_id = call_record['caller_id']\n",
        "        callee_id = call_record['callee_id']\n",
        "        caller_cell_id = call_record['cell_id']\n",
        "\n",
        "        # Get caller's location using the cell ID for each call record\n",
        "        caller_location = antenna_locations.get(caller_cell_id, (np.nan, np.nan))\n",
        "\n",
        "        # Get callee's home location if available\n",
        "        callee_homes = home_locations.get(callee_id, None)\n",
        "\n",
        "        if callee_homes is not None:\n",
        "            callee_location = antenna_locations.get(callee_homes[0], (np.nan, np.nan))\n",
        "\n",
        "            # Proceed if both locations are valid\n",
        "            if not np.isnan(caller_location[0]) and not np.isnan(caller_location[1]) and \\\n",
        "               not np.isnan(callee_location[0]) and not np.isnan(callee_location[1]):\n",
        "\n",
        "                distance = haversine(*caller_location, *callee_location)\n",
        "\n",
        "                if distance == 0:\n",
        "                    distance = 0.5\n",
        "\n",
        "                    # FOR Distribution analysis\n",
        "                #     distance_nozero = 0.1\n",
        "                # else:\n",
        "                #     distance_nozero = distance\n",
        "\n",
        "                # Initialize call weight\n",
        "                call_weight = 1  # Assuming each call record is weighted equally\n",
        "\n",
        "                # Add distance and weight to caller's record\n",
        "                if caller_id not in caller_distances:\n",
        "                    caller_distances[caller_id] = {'distances': [], 'weights': []}\n",
        "                caller_distances[caller_id]['distances'].append(distance * call_weight)\n",
        "                caller_distances[caller_id]['weights'].append(call_weight)\n",
        "\n",
        "                # FOR Distribution analysis\n",
        "                # caller_distances[caller_id]['distances_nozero'].append(distance_nozero * call_weight)\n",
        "        else:\n",
        "            callee_miss_home_record += 1\n",
        "\n",
        "    # Calculate the network radius for each caller\n",
        "    for caller_id, data in caller_distances.items():\n",
        "        total_weight = sum(data['weights'])\n",
        "        if total_weight > 0:\n",
        "            individual_radius = sum(data['distances']) / total_weight\n",
        "\n",
        "            # FOR Distribution analysis\n",
        "            # individual_radius_nozero = sum(data['distances_nozero']) / total_weight\n",
        "            # individual_radii_nozero.append(individual_radius_nozero)\n",
        "\n",
        "            if is_log:\n",
        "                individual_radius = np.log1p(individual_radius)\n",
        "\n",
        "            if individual_radius == 0:\n",
        "                zero_radius_count += 1\n",
        "                if not non_zero:  # Only append if non_zero is False\n",
        "                    individual_radii.append(individual_radius)\n",
        "            else:\n",
        "                individual_radii.append(individual_radius)\n",
        "            total_weight_final += total_weight\n",
        "\n",
        "    # Calculate the average network radius\n",
        "    network_radius = np.mean(individual_radii) if individual_radii else 0\n",
        "    total_users_final = len(individual_radii)\n",
        "\n",
        "    # Plot the distribution of individual network radii\n",
        "    # plot_network_radius_distribution(individual_radii, individual_radii_nozero, city, output_suffix)\n",
        "\n",
        "    return network_radius, total_weight_final, total_users_final, callee_miss_home_record, zero_radius_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsZRhaKq7vl2"
      },
      "source": [
        "## Method 4: Utility Functions for using duration as the weight and actual location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Canx3P8N7ss8"
      },
      "outputs": [],
      "source": [
        "def read_cdr_count_cell_dura(file_path_cdr):\n",
        "    \"\"\"\n",
        "    Read the CDR file and return a dictionary of user call records,\n",
        "    significant user call records based on time conditions,\n",
        "    and calls between users.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dictionary to hold significant user call records based on time conditions to calculate the home location\n",
        "    calls_users_time_filter = {}\n",
        "    # List to store the call record\n",
        "    calls_between_users = []\n",
        "    # Spam call possibility\n",
        "    dura_spam = 0\n",
        "\n",
        "    # Read the data file line by line\n",
        "    with open(file_path_cdr, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into parts\n",
        "            parts = line.strip().split(';')\n",
        "\n",
        "            duration = float(parts[2])\n",
        "            if duration <= 0.1:\n",
        "                dura_spam += 1\n",
        "                continue\n",
        "\n",
        "            # Extract the columns of interest: Date, Time, Caller ID, Cell ID\n",
        "            date_time_str = parts[0] + ' ' + parts[1]\n",
        "            caller_id = parts[4]\n",
        "            callee_id = parts[6]\n",
        "            cell_id = parts[7]\n",
        "\n",
        "            # Parse date and time\n",
        "            date_time = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S')\n",
        "            weekday = date_time.weekday()\n",
        "            hour = date_time.hour\n",
        "\n",
        "            # Store calls between users\n",
        "            calls_between_users.append({\n",
        "                'caller_id': caller_id,\n",
        "                'callee_id': callee_id,\n",
        "                'cell_id': cell_id,\n",
        "                'duration': duration\n",
        "            })\n",
        "\n",
        "            # Filter based on time conditions\n",
        "            if (weekday < 5 and (hour >= 19 or hour <= 6)) or (weekday >= 5):\n",
        "                if caller_id not in calls_users_time_filter:\n",
        "                    calls_users_time_filter[caller_id] = {}\n",
        "\n",
        "                # Count calls per location for significant times\n",
        "                if cell_id not in calls_users_time_filter[caller_id]:\n",
        "                    calls_users_time_filter[caller_id][cell_id] = 1\n",
        "                else:\n",
        "                    calls_users_time_filter[caller_id][cell_id] += 1\n",
        "    return calls_users_time_filter, calls_between_users, dura_spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVgvwyRK8nbk"
      },
      "outputs": [],
      "source": [
        "def network_radius_calculation_cell_dura(calls_between_users, home_locations, antenna_locations, city, output_suffix, is_log=False, non_zero=False):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on the calls between users using the caller's cell location\n",
        "    and the callee's home location, then average.\n",
        "\n",
        "    Args:\n",
        "        calls_between_users: List of call records with caller_id, callee_id, and cell_id.\n",
        "        home_locations: Dictionary mapping users to their home location (cell).\n",
        "        antenna_locations: Dictionary mapping cell IDs to (latitude, longitude).\n",
        "        city: The name of the city for saving plots.\n",
        "        is_log: Whether to apply a log transform to the call weights.\n",
        "        non_zero: Whether to exclude zero individual radii from the final calculation.\n",
        "\n",
        "    Returns:\n",
        "        Average network radius for the city, total weight, number of final users, and number of users with missing home locations.\n",
        "    \"\"\"\n",
        "    # Initialize variables\n",
        "    individual_radii = []\n",
        "    total_weight_final = 0.0\n",
        "    callee_miss_home_record = 0\n",
        "    zero_radius_count = 0\n",
        "\n",
        "    # FOR Distribution analysis\n",
        "    # individual_radii_nozero = []\n",
        "\n",
        "    # Create a dictionary to hold distances and weights per caller\n",
        "    caller_distances = {}\n",
        "\n",
        "    # Calculate network radius for each caller\n",
        "    for call_record in calls_between_users:\n",
        "        caller_id = call_record['caller_id']\n",
        "        callee_id = call_record['callee_id']\n",
        "        caller_cell_id = call_record['cell_id']\n",
        "        call_weight = call_record['duration']\n",
        "\n",
        "        # Get caller's location using the cell ID for each call record\n",
        "        caller_location = antenna_locations.get(caller_cell_id, (np.nan, np.nan))\n",
        "\n",
        "        # Get callee's home location if available\n",
        "        callee_homes = home_locations.get(callee_id, None)\n",
        "\n",
        "        if callee_homes is not None:\n",
        "            callee_location = antenna_locations.get(callee_homes[0], (np.nan, np.nan))\n",
        "\n",
        "            # Proceed if both locations are valid\n",
        "            if not np.isnan(caller_location[0]) and not np.isnan(caller_location[1]) and \\\n",
        "               not np.isnan(callee_location[0]) and not np.isnan(callee_location[1]):\n",
        "\n",
        "                distance = haversine(*caller_location, *callee_location)\n",
        "\n",
        "                if distance == 0:\n",
        "                    distance = 0.5\n",
        "\n",
        "                    # FOR Distribution analysis\n",
        "                #     distance_nozero = 0.5\n",
        "                # else:\n",
        "                #     distance_nozero = distance\n",
        "\n",
        "                # Add distance and weight to caller's record\n",
        "                if caller_id not in caller_distances:\n",
        "                    caller_distances[caller_id] = {'distances': [], 'weights': []}\n",
        "                caller_distances[caller_id]['distances'].append(distance * call_weight)\n",
        "                caller_distances[caller_id]['weights'].append(call_weight)\n",
        "\n",
        "                # FOR Distribution analysis\n",
        "                # caller_distances[caller_id]['distances_nozero'].append(distance_nozero * call_weight)\n",
        "\n",
        "        else:\n",
        "            callee_miss_home_record += 1\n",
        "\n",
        "    # Calculate the network radius for each caller\n",
        "    for caller_id, data in caller_distances.items():\n",
        "        total_weight = sum(data['weights'])\n",
        "        if total_weight > 0:\n",
        "            individual_radius = sum(data['distances']) / total_weight\n",
        "\n",
        "            # FOR Distribution analysis\n",
        "            # individual_radius_nozero = sum(data['distances_nozero']) / total_weight\n",
        "            # individual_radii_nozero.append(individual_radius_nozero)\n",
        "\n",
        "            if is_log:\n",
        "                individual_radius = np.log1p(individual_radius)\n",
        "\n",
        "            if individual_radius == 0:\n",
        "                zero_radius_count += 1\n",
        "                if not non_zero:  # Only append if non_zero is False\n",
        "                    individual_radii.append(individual_radius)\n",
        "            else:\n",
        "                individual_radii.append(individual_radius)\n",
        "\n",
        "            total_weight_final += total_weight\n",
        "\n",
        "    # Calculate the average network radius\n",
        "    network_radius = np.mean(individual_radii) if individual_radii else 0\n",
        "    total_users_final = len(individual_radii)\n",
        "\n",
        "    # Plot the distribution of individual network radii\n",
        "    # plot_network_radius_distribution(individual_radii, individual_radii_nozero, city, output_suffix)\n",
        "\n",
        "    return network_radius, total_weight_final, total_users_final, callee_miss_home_record, zero_radius_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK03IrWNrcgd"
      },
      "source": [
        "# Main Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7sbsLGGuMD"
      },
      "source": [
        "# Method 1 to calculate the radius\n",
        "\n",
        "\n",
        "\n",
        "1.   Home location for both callers and callees;\n",
        "2.   Call counts as the weight.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wyUPb2dGXYj"
      },
      "source": [
        "## Distribution analysis for weight(distance here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT_H4XrUTmKE"
      },
      "outputs": [],
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    # excluded_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_between_users = read_cdr_count(file_path_cdr)\n",
        "\n",
        "    df_calls = convert_dict_to_df_count(calls_users_time_filter)\n",
        "\n",
        "    # Plotting the KDE (Kernel Density Estimate)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if not df_calls['total_call_weight'].empty:\n",
        "        sns.histplot(df_calls['total_call_weight'], bins=30, kde=False)\n",
        "        plt.title(f'Frequency Distribution of Call Counts in {city}')\n",
        "        plt.xlabel('Call Counts')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Calculate and display statistical summary\n",
        "        desc_stats = df_calls['total_call_weight'].describe()\n",
        "        stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in desc_stats.items()])\n",
        "        # stats_text += f'\\nNormality Test: {normality_result} (p={p:.3f})'\n",
        "        plt.figtext(0.35, 0.4, f'Statistical Summary:\\n{stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "        # Save the KDE plot\n",
        "        kde_path = os.path.join('/content/drive/MyDrive/network_radius_city_size/distribution analysis/call counts/all range', f'Call_Counts_{city}.png')\n",
        "        plt.savefig(kde_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(f\"No call weight data available for {city}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2aSELFeGgwp"
      },
      "source": [
        "## Main Loop of Method 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nx214myfRBt"
      },
      "outputs": [],
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "is_log = True\n",
        "non_zero = True\n",
        "output_suffix = f\"call_counts_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_home\"\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    # excluded_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_between_users, dura_spam = read_cdr_count(file_path_cdr)\n",
        "\n",
        "    # Determine significant users\n",
        "    significant_users = significant_users_calculation(calls_users_time_filter)\n",
        "\n",
        "    # Determine the home location based on the filtered records\n",
        "    home_locations = home_location_calculation(calls_users_time_filter, significant_users)\n",
        "\n",
        "    count_antenna = len(antenna_locations)\n",
        "    count_significant_users = len(significant_users)\n",
        "    filter_users_1 = len(calls_users_time_filter)\n",
        "    total_users = len(calls_between_users)\n",
        "    percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "    percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "    total_calls_record = sum(sum(calls.values()) for calls in calls_between_users.values()) + dura_spam\n",
        "    percentage_dura_spam = (dura_spam / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "\n",
        "    # release calls_users_time_filter and significant_users\n",
        "    del calls_users_time_filter\n",
        "    del significant_users\n",
        "    gc.collect()\n",
        "\n",
        "    # ** Calculation methond 2 **\n",
        "    network_radius, total_weight_final, total_users_final, callee_miss_home, zero_radius_count = network_radius_calculation_individual(\n",
        "        calls_between_users, home_locations, antenna_locations, city, output_suffix, is_log=is_log, non_zero=non_zero)\n",
        "\n",
        "    del calls_between_users\n",
        "    del home_locations\n",
        "    del antenna_locations\n",
        "    gc.collect()\n",
        "\n",
        "    percentage_zero_radius_count = (zero_radius_count / total_users_final) * 100 if total_users_final else 0\n",
        "    percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "    percentage_final_calls = (total_weight_final / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_record,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home,\n",
        "            zero_radius_count,\n",
        "            percentage_zero_radius_count,\n",
        "            dura_spam,\n",
        "            percentage_dura_spam,\n",
        "            count_antenna\n",
        "            )\n",
        "\n",
        "    print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "    write_data_to_csv(df, city, city_data, output_suffix)\n",
        "    header_written = True  # Update the global variable after writing header"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-EK3pOs7EAw"
      },
      "source": [
        "# Method 2 to calculate the radius\n",
        "\n",
        "1.   both home location for callers and callees\n",
        "2.   Duration as weight\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdtxUH1iyFI7"
      },
      "source": [
        "## Distribution analysis for weight(duration here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC66ZM5UyERD"
      },
      "outputs": [],
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    # excluded_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_users_duration = read_cdr_duration(file_path_cdr)\n",
        "\n",
        "    df_calls = convert_dict_to_df_count(calls_users_duration)\n",
        "\n",
        "    # q_low = df_calls['total_call_weight'].quantile(0.1)\n",
        "    # q_high = df_calls['total_call_weight'].quantile(0.9)\n",
        "    # q_low = 5\n",
        "    # q_high = 200\n",
        "\n",
        "    # print(f'q_low: {q_low}, q_high: {q_high}')\n",
        "\n",
        "    # df_calls = df_calls[(df_calls['total_call_weight'] >= q_low) & (df_calls['total_call_weight'] <= q_high)]\n",
        "\n",
        "    stat, p = normaltest(df_calls['total_call_weight'])\n",
        "    normality_result = \"Gaussian (fail to reject H0)\" if p > 0.05 else \"not Gaussian (reject H0)\"\n",
        "    print(f'Statistics={stat:.3f}, p={p:.3f}')\n",
        "    if p > 0.05:\n",
        "        print(f'{city} looks Gaussian (fail to reject H0)')\n",
        "    else:\n",
        "        print(f'{city} does not look Gaussian (reject H0)')\n",
        "\n",
        "    # Plotting the KDE (Kernel Density Estimate)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if not df_calls['total_call_weight'].empty:\n",
        "        sns.histplot(df_calls['total_call_weight'], bins=30, kde=False)\n",
        "        plt.title(f'Frequency Distribution of Call Duration in {city}')\n",
        "        plt.xlabel('Call duration')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Calculate and display statistical summary\n",
        "        desc_stats = df_calls['total_call_weight'].describe()\n",
        "        stats_text = '\\n'.join([f'{key}: {value:.2f}' for key, value in desc_stats.items()])\n",
        "        # stats_text += f'\\nNormality Test: {normality_result} (p={p:.3f})'\n",
        "        plt.figtext(0.35, 0.4, f'Statistical Summary:\\n{stats_text}', fontsize=10, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "        # Save the KDE plot\n",
        "        kde_path = os.path.join('/content/drive/MyDrive/network_radius_city_size/distribution analysis/call duration/all range', f'Call_duration_{city}.png')\n",
        "        plt.savefig(kde_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(f\"No call weight data available for {city}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ_uHjysHwUw"
      },
      "source": [
        "## Main Loop for Method 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV2n2o_8n0S7"
      },
      "outputs": [],
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "is_log = True\n",
        "non_zero = True\n",
        "output_suffix = f\"dura_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_home\"\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # Check if cdr and ana files are in the folders\n",
        "    if cdr_file_name not in os.listdir(folder_path_cdr):\n",
        "        print(f\"File {cdr_file_name} not found in the folder.\")\n",
        "        continue\n",
        "\n",
        "    if ana_file_name not in os.listdir(folder_path_ana):\n",
        "        print(f\"File {ana_file_name} not found in the folder.\")\n",
        "        continue\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_users_duration, dura_spam = read_cdr_duration(file_path_cdr)\n",
        "\n",
        "    # Determine significant users\n",
        "    significant_users = significant_users_calculation(calls_users_time_filter)\n",
        "\n",
        "    # Determine the home location based on the filtered records\n",
        "    home_locations = home_location_calculation(calls_users_time_filter, significant_users)\n",
        "\n",
        "    count_antenna = len(antenna_locations)\n",
        "    total_users = len(calls_users_duration)\n",
        "    total_calls_weight = sum(sum(calls.values()) for calls in calls_users_duration.values())\n",
        "\n",
        "    filter_users_1 = len(calls_users_time_filter)\n",
        "    percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "\n",
        "    count_significant_users = len(significant_users)\n",
        "    percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "\n",
        "    # release calls_users_time_filter and significant_users\n",
        "    del calls_users_time_filter\n",
        "    del significant_users\n",
        "    gc.collect()\n",
        "\n",
        "    # Calculate the network radius\n",
        "    network_radius, total_weight_final, total_users_final, callee_miss_home, zero_radius_count = network_radius_calculation_individual(\n",
        "        calls_users_duration, home_locations, antenna_locations, city, output_suffix, is_log=is_log, non_zero=non_zero)\n",
        "\n",
        "    del calls_users_duration\n",
        "    del home_locations\n",
        "    del antenna_locations\n",
        "    gc.collect()\n",
        "\n",
        "    percentage_zero_radius_count = (zero_radius_count / total_users_final) * 100 if total_users_final else 0\n",
        "    percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "    percentage_final_calls = (total_weight_final / total_calls_weight) * 100 if total_calls_weight else 0\n",
        "\n",
        "    city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_weight,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home,\n",
        "            zero_radius_count,\n",
        "            percentage_zero_radius_count,\n",
        "            dura_spam,\n",
        "            0,\n",
        "            count_antenna\n",
        "            )\n",
        "\n",
        "    print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "    write_data_to_csv(df, city, city_data, output_suffix)\n",
        "    header_written = True  # Update the global variable after writing header"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FySjkrortT_P"
      },
      "source": [
        "# Method 3 to calculate the radius\n",
        "\n",
        "1.   Cell location for callers and home location for callees\n",
        "2.   Call counts as weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXjryIkAtajh"
      },
      "outputs": [],
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "\n",
        "is_log = True\n",
        "non_zero = True\n",
        "output_suffix = f\"call_counts_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_cell\"\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_between_users, dura_spam = read_cdr_count_cell(file_path_cdr)\n",
        "\n",
        "    # Determine significant users\n",
        "    significant_users = significant_users_calculation(calls_users_time_filter)\n",
        "\n",
        "    # Determine the home location based on the filtered records\n",
        "    home_locations = home_location_calculation(calls_users_time_filter, significant_users)\n",
        "\n",
        "    count_antenna = len(antenna_locations)\n",
        "    count_significant_users = len(significant_users)\n",
        "    filter_users_1 = len(calls_users_time_filter)\n",
        "    total_users = len(set(record['caller_id'] for record in calls_between_users))\n",
        "    percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "    percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "    total_calls_record = len(calls_between_users) + dura_spam\n",
        "    percentage_dura_spam = (dura_spam / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    # release calls_users_time_filter and significant_users\n",
        "    del calls_users_time_filter\n",
        "    del significant_users\n",
        "    gc.collect()\n",
        "\n",
        "    network_radius, total_weight_final, total_users_final, callee_miss_home, zero_radius_count = network_radius_calculation_cell(\n",
        "        calls_between_users=calls_between_users, home_locations=home_locations, antenna_locations=antenna_locations,\n",
        "        city=city, output_suffix=output_suffix, is_log=is_log, non_zero=non_zero)\n",
        "\n",
        "    del calls_between_users\n",
        "    del home_locations\n",
        "    del antenna_locations\n",
        "    gc.collect()\n",
        "\n",
        "    percentage_zero_radius_count = (zero_radius_count / total_users_final) * 100 if total_users_final else 0\n",
        "    percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "    percentage_final_calls = (total_weight_final / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_record,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home,\n",
        "            zero_radius_count,\n",
        "            percentage_zero_radius_count,\n",
        "            dura_spam,\n",
        "            percentage_dura_spam,\n",
        "            count_antenna\n",
        "            )\n",
        "\n",
        "    print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "    write_data_to_csv(df, city, city_data, output_suffix)\n",
        "    header_written = True  # Update the global variable after writing header"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZybNbw-KPR"
      },
      "source": [
        "# Method 4 to calculate the radius\n",
        "\n",
        "1.   Cell location for callers and home location for callees\n",
        "2.   Call duration as weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsyDg04Y-OSM"
      },
      "outputs": [],
      "source": [
        "# read cidades_CDR\n",
        "file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "df = pd.read_excel(file_path_cidades)\n",
        "df['City_lower'] = df['City'].apply(normalize_string)\n",
        "\n",
        "# loop for loading all the files in the folder\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "# file_names_ana = [file for file in os.listdir(folder_path_ana) if file.endswith('.txt')]\n",
        "\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "# take the city name in file names which is between '_' and '.txt'\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "# Indicator for first time writing csv file\n",
        "header_written = False\n",
        "is_log = True\n",
        "non_zero = True\n",
        "output_suffix = f\"dura_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_cell\"\n",
        "\n",
        "# select the corresponding file in file_names_ana when loop in the file_names_cdr\n",
        "for city in cities:\n",
        "\n",
        "    if city == 'Fortaleza':\n",
        "        continue\n",
        "\n",
        "    significant_users = set()\n",
        "    cdr_file_name = 'cdr_' + city + '.txt'\n",
        "    ana_file_name = 'antennas_' + city + '.txt'\n",
        "    file_path_cdr = os.path.join(folder_path_cdr, cdr_file_name)\n",
        "    file_path_ana = os.path.join(folder_path_ana, ana_file_name)\n",
        "\n",
        "    # read antenna locations\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # read cdr\n",
        "    calls_users_time_filter, calls_between_users, dura_spam = read_cdr_count_cell_dura(file_path_cdr)\n",
        "\n",
        "    # Determine significant users\n",
        "    significant_users = significant_users_calculation(calls_users_time_filter)\n",
        "\n",
        "    # Determine the home location based on the filtered records\n",
        "    home_locations = home_location_calculation(calls_users_time_filter, significant_users)\n",
        "\n",
        "    count_antenna = len(antenna_locations)\n",
        "    count_significant_users = len(significant_users)\n",
        "    filter_users_1 = len(calls_users_time_filter)\n",
        "    total_users = len(set(record['caller_id'] for record in calls_between_users))\n",
        "    percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "    percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "    total_calls_record = len(calls_between_users) + dura_spam\n",
        "    percentage_dura_spam = (dura_spam / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    # release calls_users_time_filter and significant_users\n",
        "    del calls_users_time_filter\n",
        "    del significant_users\n",
        "    gc.collect()\n",
        "\n",
        "    network_radius, total_weight_final, total_users_final, callee_miss_home, zero_radius_count = network_radius_calculation_cell_dura(\n",
        "        calls_between_users=calls_between_users, home_locations=home_locations, antenna_locations=antenna_locations,\n",
        "        city=city, output_suffix=output_suffix, is_log=is_log, non_zero=non_zero)\n",
        "\n",
        "    del calls_between_users\n",
        "    del home_locations\n",
        "    del antenna_locations\n",
        "    gc.collect()\n",
        "\n",
        "    percentage_zero_radius_count = (zero_radius_count / total_users_final) * 100 if total_users_final else 0\n",
        "    percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "    percentage_final_calls = (total_weight_final / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "    city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_record,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home,\n",
        "            zero_radius_count,\n",
        "            percentage_zero_radius_count,\n",
        "            dura_spam,\n",
        "            percentage_dura_spam,\n",
        "            count_antenna\n",
        "            )\n",
        "\n",
        "    print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "    write_data_to_csv(df, city, city_data, output_suffix)\n",
        "    header_written = True  # Update the global variable after writing header"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uq-rsbPr5Py"
      },
      "source": [
        "# Dataframe Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBuIynL6r9ps"
      },
      "source": [
        "## Common Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_SuYRM7uLoc"
      },
      "outputs": [],
      "source": [
        "# Read CDR data into a DataFrame\n",
        "def load_cdr_data(file_path_cdr):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "      file_path_cdr:\n",
        "\n",
        "    Returns:\n",
        "      Dataframe of CDR data\n",
        "\n",
        "    \"\"\"\n",
        "    columns_to_use = [0, 1, 2, 4, 6, 7]\n",
        "    column_names = ['date', 'time', 'duration', 'caller_id', 'callee_id', 'cell_id']\n",
        "\n",
        "    # Specify data types for each column to optimize memory usage\n",
        "    dtype = {\n",
        "        'duration': 'float32',\n",
        "        'caller_id': 'category',\n",
        "        'callee_id': 'category',\n",
        "        'cell_id': 'category'\n",
        "    }\n",
        "\n",
        "    # Read the data including only the required columns and specifying the date-time format\n",
        "    df_cdr = pd.read_csv(\n",
        "        file_path_cdr,\n",
        "        sep=';',\n",
        "        header=None,\n",
        "        usecols=columns_to_use,\n",
        "        dtype=dtype,\n",
        "        names=column_names,\n",
        "        parse_dates={'datetime': ['date', 'time']},\n",
        "        date_format='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "\n",
        "    # Add a 'weekday' and 'hour' column for time filtering\n",
        "    df_cdr['weekday'] = df_cdr['datetime'].dt.weekday\n",
        "    df_cdr['hour'] = df_cdr['datetime'].dt.hour\n",
        "\n",
        "    return df_cdr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRTuB6SkvWop"
      },
      "outputs": [],
      "source": [
        "# Filter for significant time conditions and aggregate call data\n",
        "def filter_significant_calls(df_cdr):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "      df_cdr:\n",
        "\n",
        "    Returns:\n",
        "      List of significant users and dictionary of their home locations\n",
        "\n",
        "    \"\"\"\n",
        "    # Filter based on time conditions (weekdays after 7 PM and before 6 AM, weekends all day)\n",
        "    significant_calls = df_cdr[((df_cdr['weekday'] < 5) & ((df_cdr['hour'] >= 19) | (df_cdr['hour'] <= 6))) | (df_cdr['weekday'] >= 5)]\n",
        "\n",
        "    filter_users_1 = significant_calls['caller_id'].nunique()\n",
        "\n",
        "    # Group by caller and cell_id to get call counts\n",
        "    calls_users_time_filter = significant_calls.groupby(['caller_id', 'cell_id']).size().reset_index(name='call_count')\n",
        "\n",
        "    # Identify significant users\n",
        "    significant_users = calls_users_time_filter.groupby('caller_id')['call_count'].sum()\n",
        "    significant_users = significant_users[(significant_users >= 5) & (significant_users <= 200)].index.tolist()\n",
        "\n",
        "    # Determine home locations based on most frequent cell_id for each significant user\n",
        "    home_locations = calls_users_time_filter[calls_users_time_filter['caller_id'].isin(significant_users)]\n",
        "\n",
        "    home_locations = home_locations.loc[home_locations.groupby('caller_id', observed=True)['call_count'].idxmax()]\n",
        "\n",
        "    # home_locations = home_locations.loc[home_locations.groupby('caller_id')['call_count'].idxmax()]\n",
        "\n",
        "    return home_locations.set_index('caller_id')['cell_id'].to_dict(), significant_users, filter_users_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEtxHlj3gwaL"
      },
      "outputs": [],
      "source": [
        "# Calculate the network radius\n",
        "def calculate_network_radius(df_cdr, home_locations, antenna_locations, weight='call_count', is_log=False, location='home'):\n",
        "    \"\"\"\n",
        "    Calculate the network radius based on calls between users and their home locations.\n",
        "\n",
        "    Args:\n",
        "      df_cdr: Dataframe of CDR data\n",
        "      home_locations: Dictionary of significant users and their home locations\n",
        "      antenna_locations: Dictionary of cells' locations\n",
        "      weight: 'call_count' or 'duration' to use different value as weight (default 'counts')\n",
        "      is_log: 'True' or 'False' to use log1p transform or not (default 'False')\n",
        "      location: 'home' or 'cell' to use home locations or actual location of each call record for callers,\n",
        "                always use home location for callees (default 'home')\n",
        "\n",
        "    Returns:\n",
        "      Network radius, total weight, total users, and number of users with missing home locations\n",
        "    \"\"\"\n",
        "    total_weight_final = 0.0\n",
        "\n",
        "    # Simplify dataframe by removing redundant columns\n",
        "    if location == 'home':\n",
        "        if weight == 'call_count':\n",
        "            # Group by caller and callee to get call counts, and reduce to unique pairs\n",
        "            call_counts = df_cdr.groupby(['caller_id', 'callee_id']).size().reset_index(name='call_count')\n",
        "            df_cdr = call_counts\n",
        "        else:\n",
        "            df_cdr = df_cdr[['caller_id', 'callee_id', 'duration']]\n",
        "    else:\n",
        "        if weight == 'call_count':\n",
        "            call_counts = df_cdr.groupby(['caller_id', 'callee_id', 'cell_id']).size().reset_index(name='call_count')\n",
        "            df_cdr = call_counts\n",
        "        else:\n",
        "            df_cdr = df_cdr[['caller_id', 'callee_id', 'cell_id', 'duration']]\n",
        "\n",
        "    # Add locations based on home or cell\n",
        "    def get_location(type_, row):\n",
        "        if type_ == 'caller':\n",
        "            return home_locations.get(row['caller_id']) if location == 'home' else antenna_locations.get(row['cell_id'])\n",
        "        else:\n",
        "            return home_locations.get(row['callee_id'])\n",
        "\n",
        "    # Calculate distances and filter valid calls\n",
        "    df_cdr['caller_location'] = df_cdr.apply(lambda row: get_location('caller', row), axis=1)\n",
        "    df_cdr['callee_location'] = df_cdr.apply(lambda row: get_location('callee', row), axis=1)\n",
        "\n",
        "    # Drop rows with missing locations\n",
        "    valid_calls = df_cdr.dropna(subset=['caller_location', 'callee_location'])\n",
        "\n",
        "    # Calculate distance using vectorized operations\n",
        "    valid_calls['distance'] = valid_calls.apply(\n",
        "        lambda row: haversine(*(row['caller_location']), *(row['callee_location'])),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Select weight column\n",
        "    weight_column = 'duration' if weight == 'duration' else 'call_count'\n",
        "\n",
        "    # Apply log transform if specified\n",
        "    if is_log:\n",
        "        valid_calls[weight_column] = np.log1p(valid_calls[weight_column])\n",
        "\n",
        "    # Calculate weighted distances\n",
        "    valid_calls['weighted_distance'] = valid_calls['distance'] * valid_calls[weight_column]\n",
        "    total_distance_weighted = valid_calls['weighted_distance'].sum()\n",
        "    total_weight_final = valid_calls[weight_column].sum()\n",
        "\n",
        "    # Calculate network radius\n",
        "    network_radius = total_distance_weighted / valid_calls['caller_id'].nunique() if valid_calls['caller_id'].nunique() else 0\n",
        "\n",
        "    # Count missing home locations for callees\n",
        "    unique_callees = pd.Series(df_cdr['callee_id'].unique())\n",
        "    callee_miss_home = len(unique_callees) - unique_callees.isin(home_locations.keys()).sum()\n",
        "\n",
        "    return network_radius, total_weight_final, valid_calls['caller_id'].nunique(), callee_miss_home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-fd_AG5NN5d"
      },
      "outputs": [],
      "source": [
        "# Main loop to process each city\n",
        "def process_cities(weight, is_log, location, output_suffix):\n",
        "    \"\"\"\n",
        "    Main loop to process each city with given parameters for network radius calculation.\n",
        "\n",
        "    Args:\n",
        "      weight: 'call_count' or 'duration'\n",
        "      is_log: True or False\n",
        "      location: 'home' or 'cell'\n",
        "      output_suffix: Suffix for the output file name to differentiate parameter sets\n",
        "    \"\"\"\n",
        "    # Read city data\n",
        "    file_path_cidades = '/content/drive/MyDrive/Brazilian Cities CDR/cidades_CDR.xlsx'\n",
        "    df_city = pd.read_excel(file_path_cidades)\n",
        "    df_city['City_lower'] = df_city['City'].apply(normalize_string)\n",
        "\n",
        "    folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "    folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "    file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "    cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "    header_written = False\n",
        "\n",
        "    for city in cities:\n",
        "        if city == 'Fortaleza':\n",
        "            continue\n",
        "\n",
        "        # Load data for the city\n",
        "        file_path_cdr = os.path.join(folder_path_cdr, f'cdr_{city}.txt')\n",
        "        file_path_ana = os.path.join(folder_path_ana, f'antennas_{city}.txt')\n",
        "        df_cdr = load_cdr_data(file_path_cdr)\n",
        "        antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "        # Process data\n",
        "        home_locations, significant_users, filter_users_1 = filter_significant_calls(df_cdr)\n",
        "        network_radius, total_weight_final, total_users_final, callee_miss_home = calculate_network_radius(\n",
        "            df_cdr, home_locations, antenna_locations, weight=weight, is_log=is_log, location=location\n",
        "        )\n",
        "\n",
        "        # Calculate percentages and other metrics\n",
        "        total_users = df_cdr['caller_id'].nunique()\n",
        "        count_significant_users = len(significant_users)\n",
        "        percentage_filter_1 = (filter_users_1 / total_users) * 100 if total_users else 0\n",
        "        percentage_significant_users = (count_significant_users / total_users) * 100 if total_users else 0\n",
        "        total_calls_record = df_cdr.shape[0]\n",
        "        percentage_final_users = (total_users_final / total_users) * 100 if total_users else 0\n",
        "        percentage_final_calls = (total_weight_final / total_calls_record) * 100 if total_calls_record else 0\n",
        "\n",
        "        del df_cdr, antenna_locations\n",
        "        gc.collect()\n",
        "\n",
        "        city_data = (\n",
        "            network_radius,\n",
        "            total_users,\n",
        "            filter_users_1,\n",
        "            percentage_filter_1,\n",
        "            count_significant_users,\n",
        "            percentage_significant_users,\n",
        "            total_users_final,\n",
        "            percentage_final_users,\n",
        "            total_calls_record,\n",
        "            total_weight_final,\n",
        "            percentage_final_calls,\n",
        "            callee_miss_home\n",
        "        )\n",
        "\n",
        "        print(f\"City: {city}, Network Radius: {network_radius:.2f} km\")\n",
        "\n",
        "        # Write data to CSV\n",
        "        write_data_to_csv(df_city, city, city_data, output_suffix, header_written)\n",
        "        header_written = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkeprEgtPo1c"
      },
      "outputs": [],
      "source": [
        "# Run the process for all combinations of parameters\n",
        "weights = ['call_count', 'duration']\n",
        "logs = [False, True]\n",
        "locations = ['home', 'cell']\n",
        "\n",
        "# List to store output suffixes\n",
        "output_suffixes = []\n",
        "\n",
        "for weight in weights:\n",
        "    for is_log in logs:\n",
        "        for location in locations:\n",
        "            output_suffix = f\"{weight}_{'log' if is_log else 'nolog'}_{location}\"\n",
        "            output_suffixes.append(output_suffix)\n",
        "            process_cities(weight, is_log, location, output_suffix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN2SfxpQilON"
      },
      "source": [
        "# Plot the polygon and compute the Convex Hull for cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONl72Se2ivE1"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import zipfile\n",
        "\n",
        "def compute_convex_hull(antenna_locations):\n",
        "    \"\"\"\n",
        "    Compute the Convex Hull for the given antenna locations and return both the geometry and its area.\n",
        "    \"\"\"\n",
        "    # Create a GeoDataFrame from the antenna locations\n",
        "    points = [Point(lon, lat) for lat, lon in antenna_locations.values()]\n",
        "    gdf = gpd.GeoDataFrame(geometry=points, crs=\"EPSG:4326\")  # EPSG:4326 is the CRS for WGS84\n",
        "\n",
        "    # Calculate the convex hull\n",
        "    convex_hull = gdf.unary_union.convex_hull\n",
        "\n",
        "    # Create a GeoDataFrame for the convex hull and set the CRS\n",
        "    convex_hull_gdf = gpd.GeoDataFrame(geometry=[convex_hull], crs=\"EPSG:4326\")\n",
        "\n",
        "    # Reproject to a projected coordinate system (e.g., UTM zone appropriate for Brazil)\n",
        "    convex_hull_gdf = convex_hull_gdf.to_crs(epsg=32723)  # UTM zone 23S for Brazil\n",
        "\n",
        "    # Calculate the area in square meters and convert to square kilometers\n",
        "    area_sq_km = convex_hull_gdf.geometry.area[0] / 1e6  # Convert square meters to square kilometers\n",
        "\n",
        "    return convex_hull, area_sq_km\n",
        "\n",
        "def plot_and_save_convex_hull(convex_hull, city, output_path):\n",
        "    \"\"\"\n",
        "    Plot and save the Convex Hull for a city.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    x, y = convex_hull.exterior.xy\n",
        "    plt.fill(x, y, alpha=0.5, fc='lightblue', ec='black')\n",
        "    plt.title(f'Convex Hull for {city}')\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_path, f'convex_hull_{city}.png'))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWsIcS9Tz0Jc"
      },
      "outputs": [],
      "source": [
        "def plot_shapefile_and_convex_hull(shapefile_path, convex_hull, antenna_locations, city, output_path):\n",
        "    \"\"\"\n",
        "    Plot the city's shapefile with the convex hull and antenna locations marked.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(shapefile_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(shapefile_folder)\n",
        "\n",
        "    # for file in os.listdir(os.path.join(shapefile_folder, 'AreasUrbanizadas2019_Brasil')):\n",
        "    #     if file.endswith('.shp'):\n",
        "    #         shapefile_path_shp = os.path.join(shapefile_folder, file)\n",
        "\n",
        "    # Read the shapefile\n",
        "    gdf = gpd.read_file('/content/drive/MyDrive/network_radius_city_size/shapefiles/AreasUrbanizadas2019_Brasil/AU_2022_AreasUrbanizadas2019_Brasil.shp')\n",
        "\n",
        "    # Normalize city names for comparison\n",
        "    normalized_city = normalize_string(city)\n",
        "    gdf['normalized_nome'] = gdf['NOME'].apply(normalize_string)\n",
        "\n",
        "    # Filter GeoDataFrame to include only the specific city\n",
        "    gdf_city = gdf[gdf['normalized_nome'] == normalized_city]\n",
        "\n",
        "    if gdf_city.empty:\n",
        "        print(f\"Warning: No data found for the city '{city}' in the shapefile.\")\n",
        "        return\n",
        "\n",
        "    # Plot the shapefile\n",
        "    fig, ax = plt.subplots()\n",
        "    gdf_city.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
        "\n",
        "    # Plot convex hull\n",
        "    x, y = convex_hull.exterior.xy\n",
        "    ax.fill(x, y, alpha=0.5, fc='lightblue', ec='black', label='Antenna Coverage')\n",
        "\n",
        "    # Plot antenna locations\n",
        "    lats, lons = zip(*antenna_locations.values())\n",
        "    ax.scatter(lons, lats, color='red', marker='o', s=5, label='Antenna Locations')\n",
        "\n",
        "    # Hide the axes\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Add title and legend\n",
        "    plt.title(f'Antenna Coverage and Locations in {city} with Geographic Shape')\n",
        "    # plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1))\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(os.path.join(output_path, f'shape_ana_area_{city}.png'))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dX6ogB3jEII"
      },
      "outputs": [],
      "source": [
        "# Path configurations\n",
        "folder_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/'\n",
        "folder_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/'\n",
        "\n",
        "file_names_cdr = [file for file in os.listdir(folder_path_cdr) if file.endswith('.txt')]\n",
        "\n",
        "# Extract the city name from file names\n",
        "cities = [file_name.split('_')[1].split('.')[0] for file_name in file_names_cdr]\n",
        "\n",
        "main_directory = '/content/drive/MyDrive/network_radius_city_size'\n",
        "\n",
        "is_duration = True\n",
        "is_home = True\n",
        "is_log = True\n",
        "non_zero = True\n",
        "\n",
        "output_suffix = f\"{'dura' if is_duration else 'call_counts'}_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_{'home' if is_home else 'cell'}\"\n",
        "\n",
        "subdirectory = os.path.join(main_directory, output_suffix)\n",
        "\n",
        "# Create the subdirectory if it does not exist\n",
        "os.makedirs(subdirectory, exist_ok=True)\n",
        "\n",
        "shapefile_folder = os.path.join(main_directory, 'shapefiles')\n",
        "shapefile_path = os.path.join(shapefile_folder, f'AreasUrbanizadas2019_Brasil.zip')\n",
        "\n",
        "file_path = os.path.join(subdirectory, f'network_radius_{output_suffix}.csv')\n",
        "\n",
        "# Read the existing CSV file\n",
        "# existing_data = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure 'City_lower' column is created for matching\n",
        "# if 'City_lower' not in existing_data.columns:\n",
        "    # existing_data['City_lower'] = existing_data['City'].apply(normalize_string)\n",
        "\n",
        "# Process each city\n",
        "for city in cities:\n",
        "    # Read antenna data for the city\n",
        "    file_path_ana = os.path.join(folder_path_ana, f'antennas_{city}.txt')\n",
        "    antenna_locations = read_ana(file_path_ana)\n",
        "\n",
        "    # Compute the Convex Hull\n",
        "    convex_hull, area_sq_km = compute_convex_hull(antenna_locations)\n",
        "\n",
        "    # Save the Convex Hull as a polygon image\n",
        "    # plot_and_save_convex_hull(convex_hull, city, subdirectory)\n",
        "    shape_out_folder = os.path.join(main_directory, 'shapefiles')\n",
        "    plot_shapefile_and_convex_hull(shapefile_path, convex_hull, antenna_locations, city, shape_out_folder)\n",
        "\n",
        "    # Update the DataFrame with Convex Hull area\n",
        "    # existing_data.loc[existing_data['City_lower'] == normalize_string(city), 'Convex_Hull_Area'] = area_sq_km\n",
        "\n",
        "# Remove City_lower column after processing\n",
        "# existing_data.drop(columns=['City_lower'], inplace=True)\n",
        "\n",
        "# Save the updated DataFrame\n",
        "# existing_data.to_csv(file_path, index=False)\n",
        "print(f\"Updated CSV saved to: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnAggRTjx665"
      },
      "outputs": [],
      "source": [
        "# for file in os.listdir(shapefile_folder):\n",
        "#     if file.endswith('.shp'):\n",
        "#         shapefile_path_shp = os.path.join(shapefile_folder, file)\n",
        "gdf = gpd.read_file('/content/drive/MyDrive/network_radius_city_size/shapefiles/AreasUrbanizadas2019_Brasil/AU_2022_AreasUrbanizadas2019_Brasil.shp')\n",
        "print(gdf.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isfym14Bgad6"
      },
      "outputs": [],
      "source": [
        "print(gdf.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXcvsvNAytxa"
      },
      "outputs": [],
      "source": [
        "print(gdf.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCLXj8S30IS_"
      },
      "outputs": [],
      "source": [
        "gdf_city = gdf[gdf['NOME'] == city]\n",
        "print(gdf_city.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFfHoKePgX5L"
      },
      "source": [
        "# Result Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kd5lDcWik0b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "is_duration = False\n",
        "is_home = False\n",
        "is_log = True\n",
        "non_zero = True\n",
        "\n",
        "output_suffix = f\"{'dura' if is_duration else 'call_counts'}_{'log' if is_log else 'nolog'}_{'nonzero' if non_zero else 'zero'}_{'home' if is_home else 'cell'}\"\n",
        "\n",
        "method_title = f\"{'H' if is_home else 'A'}_{'D' if is_duration else 'C'}\"\n",
        "\n",
        "# Define the main directory and subdirectory paths\n",
        "main_directory = '/content/drive/MyDrive/network_radius_city_size'\n",
        "subdirectory = os.path.join(main_directory, output_suffix)\n",
        "\n",
        "# Create the subdirectory if it does not exist\n",
        "os.makedirs(subdirectory, exist_ok=True)\n",
        "\n",
        "# Construct the full file path\n",
        "file_path = os.path.join(subdirectory, f'network_radius_{output_suffix}.csv')\n",
        "\n",
        "print(file_path+\"\\n\"+method_title)\n",
        "\n",
        "data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcxXUQsbhFMP"
      },
      "outputs": [],
      "source": [
        "numeric_data = data.select_dtypes(include=[float, int])\n",
        "numeric_data = numeric_data.rename(columns={'surfaceOfAdministrativeArea km2': 'administrative area', 'Convex_Hull_Area': 'antenna area', 'Population 2010': 'city population', 'Significant Users': 'antenna population'})\n",
        "\n",
        "cols_to_select = list(range(0, 1)) + list(range(25, 27)) + list(range(17, 18)) + list(range(4, 5)) # not include left range\n",
        "filter_data = numeric_data.iloc[:, cols_to_select]\n",
        "\n",
        "correlation_matrix = filter_data.corr()\n",
        "\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vr6XH2ShHtn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "\n",
        "# Rotate the labels on the x and y axes\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=45)\n",
        "\n",
        "plt.title(f'Correlation Matrix for {method_title}')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(subdirectory, f'correlation_matrix_{output_suffix}.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StQq7DRvhrCZ"
      },
      "outputs": [],
      "source": [
        "# Regression plot: official city size vs. social network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='surfaceOfAdministrativeArea km2', y='Network Radius', data=data)\n",
        "plt.title(f'Surface of Administrative Area vs. Network Radius with Regression Line for {method_title}')\n",
        "plt.xlabel('Administrative Area (km2)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'area_regression_{output_suffix}.png'))\n",
        "plt.show()\n",
        "\n",
        "# Regression plot: antenna converage area size vs. social network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='Convex_Hull_Area', y='Network Radius', data=data)\n",
        "plt.title(f'Antenna Coverage Area (Convex Hull) vs. Network Radius with Regression Line for {method_title}')\n",
        "plt.xlabel('Antenna Coverage Area (km2)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'antenna_area_regression_{output_suffix}.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBLhYxE19trS"
      },
      "outputs": [],
      "source": [
        "# Regression plot: official population vs. social network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='Population 2010', y='Network Radius', data=data)\n",
        "plt.title(f'Official City Population vs. Network Radius with Regression Line for {method_title}')\n",
        "plt.xlabel('Population 2010 (HAB)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'admin_popu_reg_{output_suffix}.png'))\n",
        "plt.show()\n",
        "\n",
        "# Regression plot: city size vs. social network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='Significant Users', y='Network Radius', data=data)\n",
        "plt.title(f'Antenna Coverage Population vs. Network Radius with Regression Line for {method_title}')\n",
        "plt.xlabel('Antenna Coverage Population (HAB)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'ana_popu_reg_{output_suffix}.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH6hStjzSIFU"
      },
      "outputs": [],
      "source": [
        "# Regression plot: population vs. social network radius\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "data['Normalized Population'] = scaler.fit_transform(data[['Significant Users']])\n",
        "data['Log Normalized Population'] = np.log(data['Normalized Population'] + 1)\n",
        "\n",
        "data['Normalized Population 2010'] = scaler.fit_transform(data[['Population 2010']])\n",
        "data['Log Normalized Population 2010'] = np.log(data['Normalized Population 2010'] + 1)\n",
        "\n",
        "data['Normalized Radius'] = scaler.fit_transform(data[['Network Radius']])\n",
        "data['Log Normalized Radius'] = np.log(data['Normalized Radius'] + 1)\n",
        "\n",
        "# data['Radius after area scaling'] = data['Network Radius'] / (data['Convex_Hull_Area'] / data['surfaceOfAdministrativeArea km2'])\n",
        "# data['Normalized Radius area'] = scaler.fit_transform(data[['Radius after area scaling']])\n",
        "# data['Log Normalized Radius area'] = np.log(data['Normalized Radius area'] + 1)\n",
        "\n",
        "# data['Radius after population scaling'] = data['Network Radius'] / (data['Significant Users'] / data['Population 2010'])\n",
        "# data['Normalized Radius population'] = scaler.fit_transform(data[['Radius after population scaling']])\n",
        "# data['Log Normalized Radius population'] = np.log(data['Normalized Radius population'] + 1)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='Log Normalized Population', y='Log Normalized Radius', data=data)\n",
        "plt.title(f'Log-Normalized Antenna Coverage Population vs. Network Radius with Regression Line for {method_title}')\n",
        "plt.xlabel('Log-Normalized Antenna Coverage Population (ln(HAB/<HAB>))')\n",
        "plt.ylabel('Log-Normalized Network Radius (ln(km/<km>))')\n",
        "plt.savefig(os.path.join(subdirectory, f'log_norm_popu_reg_{output_suffix}.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-B-vR_FaJyG"
      },
      "outputs": [],
      "source": [
        "# Regression plot: population vs. social network radius\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='Log Normalized Population 2010', y='Log Normalized Radius', data=data)\n",
        "plt.title(f'Log-Normalized Official Population vs. Network Radius with Regression Line for {method_title}')\n",
        "plt.xlabel('Log-Normalized Population 2010 (ln(HAB/<HAB>))')\n",
        "plt.ylabel('Log-Normalized Network Radius (ln(km/<km>))')\n",
        "plt.savefig(os.path.join(subdirectory, f'admin_log_norm_popu_reg_{output_suffix}.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlnHr-v0jm_t"
      },
      "outputs": [],
      "source": [
        "# Scatterplot: City area and Social Network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='surfaceOfAdministrativeArea km2', y='Network Radius', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['surfaceOfAdministrativeArea km2'][i],\n",
        "             y=data['Network Radius'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10)\n",
        "             )\n",
        "\n",
        "plt.title(f'Surface of Administrative Area vs. Network Radius for {method_title}')\n",
        "plt.xlabel('Administrative Area (km2)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'area_scatter_{output_suffix}.png'))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Scatter Plot for Antenna Coverage Area\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Convex_Hull_Area', y='Network Radius', data=data)\n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['Convex_Hull_Area'][i],\n",
        "             y=data['Network Radius'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10)\n",
        "             )\n",
        "\n",
        "plt.title(f'Antenna Coverage Area (Convex Hull) vs. Network Radius for {method_title}')\n",
        "plt.xlabel('Antenna Coverage Area (km2)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'ana_area_scatter_{output_suffix}.png'))\n",
        "plt.show()\n",
        "\n",
        "# Scatterplot: Population and Social Network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Population 2010', y='Network Radius', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['Population 2010'][i],\n",
        "             y=data['Network Radius'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10))\n",
        "\n",
        "plt.title(f'Official Population vs. Network Radius for {method_title}')\n",
        "plt.xlabel('Population 2010 (HAB)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'population_scatter_{output_suffix}.png'))\n",
        "plt.show()\n",
        "\n",
        "# Scatterplot: Population and Social Network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Significant Users', y='Network Radius', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['Significant Users'][i],\n",
        "             y=data['Network Radius'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10))\n",
        "\n",
        "plt.title(f'Antenna Coverage Population vs. Network Radius for {method_title}')\n",
        "plt.xlabel('Antenna Coverage Population (HAB)')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'ana_popu_sca_{output_suffix}.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBSXTWYgadUv"
      },
      "outputs": [],
      "source": [
        "# LOG Transform\n",
        "\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "# data['Normalized Population'] = scaler.fit_transform(data[['Significant Users']])\n",
        "# data['Log Normalized Population'] = np.log(data['Normalized Population'] + 1)\n",
        "\n",
        "# data['Normalized Population 2010'] = scaler.fit_transform(data[['Population 2010']])\n",
        "# data['Log Normalized Population 2010'] = np.log(data['Normalized Population 2010'] + 1)\n",
        "\n",
        "# Scatterplot: Population and Social Network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Log Normalized Population 2010', y='Network Radius', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['Log Normalized Population 2010'][i],\n",
        "             y=data['Network Radius'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10))\n",
        "\n",
        "plt.title(f'Log Normalized Population vs. Network Radius for {method_title}')\n",
        "plt.xlabel('Log Normalized Population 2010 (ln(HAB))')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'log_population_scatter_{output_suffix}.png'))\n",
        "plt.show()\n",
        "\n",
        "# Scatterplot: Population and Social Network radius\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Log Normalized Population', y='Network Radius', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['Log Normalized Population'][i],\n",
        "             y=data['Network Radius'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10))\n",
        "\n",
        "plt.title(f'Log Normalized Antenna Coverage Population vs. Network Radius for {method_title}')\n",
        "plt.xlabel('Log Normalized Antenna Coverage Population (ln(HAB))')\n",
        "plt.ylabel('Network Radius (km)')\n",
        "plt.savefig(os.path.join(subdirectory, f'log_ana_popu_sca_{output_suffix}.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvuSRvaRxUl3"
      },
      "outputs": [],
      "source": [
        "# Scatterplot: surfaceOfAdministrativeArea and Convex_Hull_Area\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='surfaceOfAdministrativeArea km2', y='Convex_Hull_Area', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['surfaceOfAdministrativeArea km2'][i],\n",
        "             y=data['Convex_Hull_Area'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10)\n",
        "             )\n",
        "\n",
        "plt.title(f'Surface of Administrative Area vs. Antenna Coverage Area (Convex Hull)')\n",
        "plt.xlabel('Administrative Area (km2)')\n",
        "plt.ylabel('Antenna Coverage Area (km2)')\n",
        "plt.savefig(os.path.join(main_directory, f'admin_ana_area_scatter.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vusrmhbuQewq"
      },
      "outputs": [],
      "source": [
        "# Scatterplot: population 2010 and Infered population in antenna areas\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Population 2010', y='Significant Users', data=data)\n",
        "\n",
        "# Label city names\n",
        "for i in range(data.shape[0]):\n",
        "    plt.text(x=data['Population 2010'][i],\n",
        "             y=data['Significant Users'][i],\n",
        "             s=data['City'][i],\n",
        "             fontdict=dict(color='red',size=10)\n",
        "             )\n",
        "\n",
        "plt.title(f'Official Population 2010 vs. Antenna Coverage Population')\n",
        "plt.xlabel('Official Population 2010 (HAB)')\n",
        "plt.ylabel('Antenna Coverage Population (HAB)')\n",
        "plt.savefig(os.path.join(main_directory, f'admin_ana_popu_scatter.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoAYD8IUrUe8"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Selection of independent and dependent variables\n",
        "X = numeric_data[['Population 2010', 'Convex_Hull_Area', 'surfaceOfAdministrativeArea km2', 'Significant Users']]\n",
        "y = numeric_data['Network Radius']\n",
        "\n",
        "# Adding a constant term\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif_data)\n",
        "\n",
        "# fit a regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Output regression results\n",
        "print(model.summary())\n",
        "\n",
        "with open(os.path.join(subdirectory, f'mix_4_summary_{output_suffix}.txt'), \"w\") as f:\n",
        "    f.write(model.summary().as_text())\n",
        "    f.write(\"\\n\\n\\n\")\n",
        "    f.write(vif_data.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-2cE0nLdYwN"
      },
      "outputs": [],
      "source": [
        "# Selection of independent and dependent variables\n",
        "X = numeric_data[['Convex_Hull_Area', 'Significant Users']]\n",
        "y = numeric_data['Network Radius']\n",
        "\n",
        "# Adding a constant term\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif_data)\n",
        "\n",
        "# fit a regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Output regression results\n",
        "print(model.summary())\n",
        "\n",
        "with open(os.path.join(subdirectory, f'actual_summary_{output_suffix}.txt'), \"w\") as f:\n",
        "    f.write(model.summary().as_text())\n",
        "    f.write(\"\\n\\n\\n\")\n",
        "    f.write(model.summary().as_latex())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUjSxh4JmgYt"
      },
      "source": [
        "# Plot the heatmap and antenna locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpRVvo8WmgYt"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "\n",
        "locations = []\n",
        "for user, cellid in home_locations.items():\n",
        "    if cellid in antenna_locations:\n",
        "        lat, lon = antenna_locations[cellid]\n",
        "        locations.append([lat, lon])\n",
        "\n",
        "map_center = [locations[0][0], locations[0][1]]\n",
        "map = folium.Map(location=map_center, zoom_start=12)\n",
        "\n",
        "HeatMap(locations).add_to(map)\n",
        "\n",
        "map.save('home_location_heatmap.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWfl1BuxmgYt"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "\n",
        "map_center = list(antenna_locations.values())[0]\n",
        "\n",
        "m = folium.Map(location=map_center, zoom_start=10)\n",
        "\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "for cellid, (lat, lon) in antenna_locations.items():\n",
        "    folium.Marker(\n",
        "        location=[lat, lon],\n",
        "        popup=str(cellid),\n",
        "        icon=folium.Icon(color='blue', icon='info-sign')\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "m.save('antennas_locations_map_with_clusters.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI7b_HjxmgYt"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap, MarkerCluster\n",
        "\n",
        "# Collect latitudes and longitudes for all home locations for the heatmap\n",
        "locations = []\n",
        "\n",
        "# Calculate the number of occurrences of each cellid to be used as the number of users\n",
        "cellid_counts = {}\n",
        "\n",
        "for user, cellid in home_locations.items():\n",
        "    if cellid in antenna_locations:\n",
        "        lat, lon = antenna_locations[cellid]\n",
        "        locations.append([lat, lon])\n",
        "        cellid_counts[cellid] = cellid_counts.get(cellid, 0) + 1\n",
        "\n",
        "# If there are home locations, choose the first one as the map center; otherwise, choose the first antenna locations\n",
        "map_center = locations[0] if locations else list(antenna_locations.values())[0]\n",
        "\n",
        "# Create a folium map instance\n",
        "m = folium.Map(location=map_center, zoom_start=12)\n",
        "\n",
        "# Add a heatmap layer for home locations\n",
        "HeatMap(locations).add_to(m)\n",
        "\n",
        "# Create a MarkerCluster object\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add markers for all antenna locations\n",
        "for cellid, (lat, lon) in antenna_locations.items():\n",
        "    # Get the users number using cellid_counts\n",
        "    users_count = cellid_counts.get(cellid, 0)\n",
        "\n",
        "    popup_text = f\"Cell ID: {cellid}<br>Users: {users_count}\"\n",
        "    folium.Marker(\n",
        "        location=[lat, lon],\n",
        "        popup=popup_text,  # Popup shows the popup_text\n",
        "        icon=folium.Icon(color='blue', icon='info-sign')  # Customize marker style\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "m.save('map_with_heatmap_and_clusters.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9qiqw3BC3ps"
      },
      "source": [
        "# Calculate home location, social size and plot with filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1_b6vpnPF2r"
      },
      "source": [
        "### Extract call records without filter and with filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmHTXEBSP3ye"
      },
      "source": [
        "### Caculate the social range for each user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRbzd5crKGYk"
      },
      "outputs": [],
      "source": [
        "# TODO: Do we need to filter the call made from outside the home location?\n",
        "\n",
        "# Social range\n",
        "social_ranges = {}\n",
        "\n",
        "# Dictionary to hold the distance between\n",
        "network_edges = {}\n",
        "\n",
        "for line in open(file_path_cdr_caucaia, 'r'):\n",
        "    parts = line.strip().split(';')\n",
        "    caller_id = parts[4]\n",
        "    called_id = parts[6]\n",
        "\n",
        "    if caller_id in home_locations and called_id in home_locations:\n",
        "        caller_home = home_locations[caller_id]\n",
        "        called_home = home_locations[called_id]\n",
        "        if caller_home in antenna_locations and called_home in antenna_locations:\n",
        "            lat1, lon1 = antenna_locations[caller_home]\n",
        "            lat2, lon2 = antenna_locations[called_home]\n",
        "            distance = haversine(lat1, lon1, lat2, lon2)\n",
        "\n",
        "            if caller_id not in network_edges:\n",
        "                network_edges[caller_id] = {}\n",
        "            if called_id not in network_edges[caller_id]:\n",
        "                network_edges[caller_id][called_id] = distance\n",
        "            else:\n",
        "                network_edges[caller_id][called_id] += distance\n",
        "\n",
        "# Sum distances and calculate average range\n",
        "for caller, calls in network_edges.items():\n",
        "    total_distance = sum(calls.values())\n",
        "    total_calls = sum(user_calls[caller].values())\n",
        "    social_ranges[caller] = total_distance / total_calls if total_calls else 0\n",
        "\n",
        "# Output results\n",
        "for user, range_km in social_ranges.items():\n",
        "    print(f'User {user} has a social range of {range_km:.2f} km')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0pGZj6AmgYh"
      },
      "source": [
        "## 1.1 Extract each user's call records {(User ID -> {CELLID -> Number of Calls}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUuhSpUymgYi"
      },
      "source": [
        "#### 1.3 Plot the distribution and boxplot of the total number of phone call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqUe4ZjdmgYi"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution\n",
        "total_calls_users = [sum(calls.values()) for calls in user_calls.values()]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(total_calls_users, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Total Call Counts per User')\n",
        "plt.xlabel('Total Call Counts')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "# plt.xscale('log')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb90tk0amgYi"
      },
      "outputs": [],
      "source": [
        "plt.boxplot(total_calls_users, vert=True, patch_artist=True,\n",
        "            flierprops=dict(marker='o', markerfacecolor='red', markersize=3),\n",
        "            showmeans=True)\n",
        "\n",
        "plt.title('Boxplot of Total Call Counts per User')\n",
        "plt.ylabel('Total Call Counts')\n",
        "# plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqP5OIJkmgYj"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(total_calls_users).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQfBRVsmmgYj"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of unique callees per caller\n",
        "unique_calls_per_user = {user: len(calls) for user, calls in social_network.items()}\n",
        "\n",
        "# Count how many users called exactly 1, 2, 3, 4, 5 unique users\n",
        "call_count_distribution = collections.Counter(unique_calls_per_user.values())\n",
        "\n",
        "print(f\"We have totally {len(unique_calls_per_user)} users who made call.\")\n",
        "\n",
        "total_1_to_5 = sum(call_count_distribution[i] for i in range(1, 6))\n",
        "print(f\"Number of users calling between 1 and 5 unique people: {total_1_to_5}\")\n",
        "\n",
        "percentage_1_to_5 = (total_1_to_5 / total_users) * 100\n",
        "print(f\"Percentage of users calling between 1 and 5 unique people: {percentage_1_to_5:.2f}%\")\n",
        "\n",
        "for i in range(1, 6):\n",
        "    print(f\"Number of unique users call {i} unique people: {call_count_distribution[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjTWX3VXgogU"
      },
      "source": [
        "# Dataframe analysis for smaller cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F-XGi60gugV"
      },
      "outputs": [],
      "source": [
        "# read CDR data to daraframe\n",
        "file_path_cdr = '/content/drive/MyDrive/Brazilian Cities CDR/CDR/cdr_Caucaia.txt'\n",
        "\n",
        "# Define the indices and names of the columns to be used\n",
        "columns_to_use = [0, 1, 2, 4, 6, 7]  # Adjust according to your actual data structure\n",
        "column_names = ['date', 'time', 'duration', 'caller_id', 'callee_id', 'cell_id']\n",
        "\n",
        "# Specify data types for each column to optimize memory usage\n",
        "dtype = {\n",
        "    'duration': 'float32',  # Assuming duration is a floating number\n",
        "    'caller_id': 'category',  # Use 'category' data type to reduce memory usage for repetitive strings\n",
        "    'callee_id': 'category',\n",
        "    'cell_id': 'category'\n",
        "}\n",
        "\n",
        "# Read the data including only the required columns and specifying the date-time format\n",
        "df_cdr = pd.read_csv(\n",
        "    file_path_cdr,\n",
        "    sep=';',\n",
        "    header=None,\n",
        "    usecols=columns_to_use,\n",
        "    dtype=dtype,\n",
        "    names=column_names,\n",
        "    parse_dates=[['date', 'time']],\n",
        "    date_format='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "print(df_cdr.head())\n",
        "\n",
        "# read antenna locations\n",
        "# file_path_ana = '/content/drive/MyDrive/Brazilian Cities CDR/Antena/antennas_Franca.txt'\n",
        "# df_ana = read_ana(file_path_ana)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-Svwq_cjZV5"
      },
      "outputs": [],
      "source": [
        "df_cdr.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7g74IPFjv3d"
      },
      "outputs": [],
      "source": [
        "start_date = df_cdr['date_time'].min()\n",
        "end_date = df_cdr['date_time'].max()\n",
        "total_days = (end_date - start_date).days + 1\n",
        "\n",
        "print(f\"Date range: {start_date} to {end_date}, {total_days} days in total.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQNWzItcojEK"
      },
      "outputs": [],
      "source": [
        "# Group data by day and sum durations\n",
        "daily_duration = df_cdr.groupby(df_cdr['date_time'].dt.date)['duration'].sum()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "daily_duration.plot(kind='bar', color='blue', alpha=0.7)\n",
        "\n",
        "plt.title('Total Call Duration Over 30 Days')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Duration in Minutes')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate date labels for better visibility\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGNdhwKKpCEP"
      },
      "outputs": [],
      "source": [
        "# Add a 'day_of_week' column to the DataFrame\n",
        "df_cdr['day_of_week'] = df_cdr['date_time'].dt.dayofweek\n",
        "\n",
        "# Group data by 'day_of_week' and sum durations\n",
        "weekly_duration = df_cdr.groupby('day_of_week')['duration'].sum()\n",
        "\n",
        "# Map the 'day_of_week' integers to day names for clearer x-axis labels\n",
        "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
        "weekly_duration.index = weekly_duration.index.map(day_names)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "weekly_duration.plot(kind='bar', color='blue', alpha=0.7)\n",
        "\n",
        "plt.title('Total Call Duration by Day of the Week')\n",
        "plt.xlabel('Days of the Week')\n",
        "plt.ylabel('Total Duration in Minutes')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate day labels for better visibility\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPegNwWLpvnD"
      },
      "outputs": [],
      "source": [
        "# Group data by day and count the number of calls\n",
        "daily_calls = df_cdr.groupby(df_cdr['date_time'].dt.date).size()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "daily_calls.plot(kind='bar', color='orange', alpha=0.7)\n",
        "\n",
        "plt.title('Total Call Counts Over 30 Days')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Calls')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate date labels for better visibility\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qB6wnHhqSMd"
      },
      "outputs": [],
      "source": [
        "# Group data by 'day_of_week' and count calls\n",
        "weekly_calls = df_cdr.groupby('day_of_week').size()\n",
        "\n",
        "# Map the 'day_of_week' integers to day names for clearer x-axis labels\n",
        "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
        "weekly_calls.index = weekly_calls.index.map(day_names)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "weekly_calls.plot(kind='bar', color='orange', alpha=0.7)\n",
        "\n",
        "plt.title('Total Call Counts by Day of the Week')\n",
        "plt.xlabel('Days of the Week')\n",
        "plt.ylabel('Number of Calls')\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate day labels for better visibility\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTW3q0sLqmla"
      },
      "outputs": [],
      "source": [
        "# Define bins for the duration intervals\n",
        "bins = [0, 1, 5, 10, 30, 60, 120, 240, 480, float('inf')]  # Adjust the bins as needed\n",
        "labels = ['0-1 min', '1-5 mins', '5-10 mins', '10-30 mins', '30-60 mins', '1-2 hrs', '2-4 hrs', '4-8 hrs', 'Over 8 hrs']\n",
        "\n",
        "# Categorize durations into bins\n",
        "df_cdr['duration_category'] = pd.cut(df_cdr['duration'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Count the number of calls in each duration category\n",
        "duration_distribution = df_cdr['duration_category'].value_counts().sort_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = duration_distribution.plot(kind='bar', color='blue')\n",
        "plt.title('Distribution of Call Durations')\n",
        "plt.xlabel('Duration Intervals')\n",
        "plt.ylabel('Number of Calls')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adding text labels above the bars\n",
        "for bar in bars.patches:\n",
        "    # Using bar.get_height() to get the height of the bar,\n",
        "    # and bar.get_x() + bar.get_width() / 2 to calculate center position of the bar\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{int(bar.get_height())}',\n",
        "             ha='center', va='bottom', color='black', fontsize=9)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6K1E5ILuTTN"
      },
      "outputs": [],
      "source": [
        "# Filter calls that last less than 60 seconds\n",
        "short_calls = df_cdr[df_cdr['duration'] < 1]\n",
        "\n",
        "# Plotting the distribution of call durations under one minute\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(short_calls['duration'], bins=30, color='blue', kde=True)  # Bins for every 2 seconds\n",
        "plt.title('Distribution of Call Durations Under One Minute')\n",
        "plt.xlabel('Duration in Seconds')\n",
        "plt.ylabel('Number of Calls')\n",
        "plt.xticks([i/60 for i in range(0, 61, 5)], [f\"{i}s\" for i in range(0, 61, 5)])  # Setting x-ticks to show seconds\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYFlDCHTBVgo"
      },
      "outputs": [],
      "source": [
        "# Define bins for the duration intervals\n",
        "bins = [0, 1, 5, 10, 30, 60, 120, 240, 480, float('inf')]  # Adjust the bins as needed\n",
        "labels = ['0-1 min', '1-5 mins', '5-10 mins', '10-30 mins', '30-60 mins', '1-2 hrs', '2-4 hrs', '4-8 hrs', 'Over 8 hrs']\n",
        "\n",
        "# Categorize durations into bins\n",
        "df_cdr['duration_category'] = pd.cut(df_cdr['duration'], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Create a box plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='duration_category', y='duration', data=df_cdr, palette='Blues')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Box Plot of Call Durations')\n",
        "plt.xlabel('Duration Intervals')\n",
        "plt.ylabel('Call Duration (minutes)')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYmGut_ltKuQ"
      },
      "outputs": [],
      "source": [
        "# Sum durations per caller_id\n",
        "total_durations = df_cdr.groupby('caller_id')['duration'].sum()\n",
        "\n",
        "# Plotting the total durations. Since there are potentially millions of callers,\n",
        "# we use a histogram and a KDE plot for a better understanding of distribution.\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(total_durations, bins=100, kde=True, color='green')\n",
        "plt.title('Distribution of Total Call Duration Per User')\n",
        "plt.xlabel('Total Duration (minutes)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xscale('log')  # Use logarithmic scale if the range is wide\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbrMug5hfzqh"
      },
      "outputs": [],
      "source": [
        "# Assuming df_cdr is your DataFrame and it includes a 'caller_id' column\n",
        "call_counts = df_cdr['caller_id'].value_counts()\n",
        "\n",
        "# Convert the Series to a DataFrame for easier handling\n",
        "call_counts_df = call_counts.reset_index()\n",
        "call_counts_df.columns = ['caller_id', 'number_of_calls']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_IOTRZ3f53Z"
      },
      "outputs": [],
      "source": [
        "# Plotting the distribution of call counts\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(call_counts, bins=100, color='blue', range=(1, call_counts.quantile(0.99)))  # Adjust range and bins as needed\n",
        "plt.title('Distribution of Call Counts per User')\n",
        "plt.xlabel('Number of Calls')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xscale('log')  # Use logarithmic scale if the data spans several orders of magnitude\n",
        "# plt.yscale('log')  # Log scale for y-axis can also be helpful in skewed distributions\n",
        "plt.show()\n",
        "\n",
        "# Show basic statistics\n",
        "print(call_counts_df['number_of_calls'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os3OJjHVgSrp"
      },
      "outputs": [],
      "source": [
        "# Adding a small constant if there are any zero call counts (unlikely in this case)\n",
        "call_counts_log = np.log1p(call_counts)\n",
        "\n",
        "# Plotting the log-transformed data\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(call_counts_log, bins=50, kde=False, color='green')\n",
        "plt.title('Log-Transformed Distribution of Call Counts per User')\n",
        "plt.xlabel('Log of Number of Calls')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7x5JMZXhQqa"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 6))  # Setup the plot size\n",
        "stats.probplot(call_counts_log, dist=\"norm\", plot=ax)  # Generate and plot the Q-Q plot\n",
        "\n",
        "# Customize the plot\n",
        "ax.get_lines()[0].set_color('blue')  # Change the color of the points to blue\n",
        "ax.get_lines()[1].set_color('red')   # Change the color of the line to red\n",
        "ax.set_title('Q-Q plot of Log-Transformed Call Durations')\n",
        "ax.set_xlabel('Theoretical Quantiles')\n",
        "ax.set_ylabel('Ordered Values')\n",
        "\n",
        "plt.show()  # Display the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odjWDcnrlTXY"
      },
      "outputs": [],
      "source": [
        "# Define the threshold for call counting\n",
        "lower_threshold = 5\n",
        "upper_threshold = 200\n",
        "\n",
        "# Apply thresholds\n",
        "filtered_call_counts = df_cdr['caller_id'].value_counts()\n",
        "total_calls_original = df_cdr['caller_id'].value_counts().sum()\n",
        "filtered_call_counts = filtered_call_counts[(filtered_call_counts >= lower_threshold) & (filtered_call_counts <= upper_threshold)]\n",
        "filtered_log_counts = np.log1p(filtered_call_counts)\n",
        "\n",
        "total_calls_filtered = filtered_call_counts.sum()\n",
        "\n",
        "percentage_filtered = (total_calls_filtered / total_calls_original) * 100\n",
        "\n",
        "print(f\"Percentage of calls remaining after filtering: {percentage_filtered:.2f}%\")\n",
        "\n",
        "# Re-plotting histograms and Q-Q plots\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(filtered_log_counts, kde=True)\n",
        "plt.title('Filtered Histogram of Log-Transformed Call Counts')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "stats.probplot(filtered_log_counts, dist=\"norm\", plot=plt)\n",
        "plt.show()\n",
        "\n",
        "# D'Agostino's K^2 test\n",
        "k2, p = stats.normaltest(filtered_log_counts)\n",
        "print(f\"Statistic: {k2}, p-value: {p}\")\n",
        "\n",
        "# Shapiro-Wilk test\n",
        "stat, p = stats.shapiro(filtered_log_counts.sample(5000))\n",
        "print(f\"Shapiro-Wilk Test: Statistic={stat}, p-value={p}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eae-JUFNmqbd"
      },
      "outputs": [],
      "source": [
        "call_counts = df_cdr['caller_id'].value_counts()\n",
        "\n",
        "# Define different thresholds for testing\n",
        "thresholds = range(1, 15)\n",
        "results = []\n",
        "\n",
        "for lower_threshold in thresholds:\n",
        "    # Applying thresholds to filter data\n",
        "    filtered_call_counts = call_counts[call_counts >= lower_threshold]\n",
        "    filtered_log_counts = np.log1p(filtered_call_counts)\n",
        "\n",
        "    # Calculating statistical tests\n",
        "    k2, p_normaltest = stats.normaltest(filtered_log_counts)\n",
        "    stat_shapiro, p_shapiro = stats.shapiro(filtered_log_counts.sample(min(5000, len(filtered_log_counts))))  # 样本量过大时需要抽样\n",
        "\n",
        "    # Storing results\n",
        "    results.append({\n",
        "        'lower_threshold': lower_threshold,\n",
        "        'p_value_normaltest': p_normaltest,\n",
        "        'p_value_shapiro': p_shapiro\n",
        "    })\n",
        "\n",
        "# Converting results to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Plotting p-values against thresholds\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(results_df['lower_threshold'], results_df['p_value_normaltest'], label='D\\'Agostino\\'s K^2 Test p-value', marker='o')\n",
        "plt.plot(results_df['lower_threshold'], results_df['p_value_shapiro'], label='Shapiro-Wilk Test p-value', marker='x')\n",
        "plt.xlabel('Lower Threshold')\n",
        "plt.ylabel('p-value')\n",
        "plt.title('Effect of Lower Threshold on Normality Tests')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zU7sbsLGGuMD",
        "B-EK3pOs7EAw",
        "cdtxUH1iyFI7",
        "eJ_uHjysHwUw",
        "FySjkrortT_P",
        "2WZybNbw-KPR",
        "iBuIynL6r9ps",
        "yN2SfxpQilON",
        "gUjSxh4JmgYt",
        "m9qiqw3BC3ps",
        "zmHTXEBSP3ye",
        "cjTWX3VXgogU"
      ],
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
